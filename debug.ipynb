{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet 2d condition model\n",
    "\n",
    "[unet](https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight torch.Size([320, 4, 3, 3])\n",
      "conv_in.bias torch.Size([320])\n",
      "time_embedding.linear_1.weight torch.Size([1280, 320])\n",
      "time_embedding.linear_1.bias torch.Size([1280])\n",
      "time_embedding.linear_2.weight torch.Size([1280, 1280])\n",
      "time_embedding.linear_2.bias torch.Size([1280])\n",
      "down_blocks.0.attentions.0.norm.weight torch.Size([320])\n",
      "down_blocks.0.attentions.0.norm.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.proj_in.weight torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.0.proj_in.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([2560, 320])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([2560])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([320, 1280])\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([320])\n",
      "down_blocks.0.attentions.0.proj_out.weight torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.0.proj_out.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.norm.weight torch.Size([320])\n",
      "down_blocks.0.attentions.1.norm.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.proj_in.weight torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.1.proj_in.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([320, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([2560, 320])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([2560])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([320, 1280])\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([320])\n",
      "down_blocks.0.attentions.1.proj_out.weight torch.Size([320, 320, 1, 1])\n",
      "down_blocks.0.attentions.1.proj_out.bias torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm1.weight torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm1.bias torch.Size([320])\n",
      "down_blocks.0.resnets.0.conv1.weight torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.0.conv1.bias torch.Size([320])\n",
      "down_blocks.0.resnets.0.time_emb_proj.weight torch.Size([320, 1280])\n",
      "down_blocks.0.resnets.0.time_emb_proj.bias torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm2.weight torch.Size([320])\n",
      "down_blocks.0.resnets.0.norm2.bias torch.Size([320])\n",
      "down_blocks.0.resnets.0.conv2.weight torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.0.conv2.bias torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm1.weight torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm1.bias torch.Size([320])\n",
      "down_blocks.0.resnets.1.conv1.weight torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.1.conv1.bias torch.Size([320])\n",
      "down_blocks.0.resnets.1.time_emb_proj.weight torch.Size([320, 1280])\n",
      "down_blocks.0.resnets.1.time_emb_proj.bias torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm2.weight torch.Size([320])\n",
      "down_blocks.0.resnets.1.norm2.bias torch.Size([320])\n",
      "down_blocks.0.resnets.1.conv2.weight torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.resnets.1.conv2.bias torch.Size([320])\n",
      "down_blocks.0.downsamplers.0.conv.weight torch.Size([320, 320, 3, 3])\n",
      "down_blocks.0.downsamplers.0.conv.bias torch.Size([320])\n",
      "down_blocks.1.attentions.0.norm.weight torch.Size([640])\n",
      "down_blocks.1.attentions.0.norm.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.proj_in.weight torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.0.proj_in.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([5120, 640])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([5120])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([640, 2560])\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([640])\n",
      "down_blocks.1.attentions.0.proj_out.weight torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.0.proj_out.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.norm.weight torch.Size([640])\n",
      "down_blocks.1.attentions.1.norm.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.proj_in.weight torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.1.proj_in.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([640, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([5120, 640])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([5120])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([640, 2560])\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([640])\n",
      "down_blocks.1.attentions.1.proj_out.weight torch.Size([640, 640, 1, 1])\n",
      "down_blocks.1.attentions.1.proj_out.bias torch.Size([640])\n",
      "down_blocks.1.resnets.0.norm1.weight torch.Size([320])\n",
      "down_blocks.1.resnets.0.norm1.bias torch.Size([320])\n",
      "down_blocks.1.resnets.0.conv1.weight torch.Size([640, 320, 3, 3])\n",
      "down_blocks.1.resnets.0.conv1.bias torch.Size([640])\n",
      "down_blocks.1.resnets.0.time_emb_proj.weight torch.Size([640, 1280])\n",
      "down_blocks.1.resnets.0.time_emb_proj.bias torch.Size([640])\n",
      "down_blocks.1.resnets.0.norm2.weight torch.Size([640])\n",
      "down_blocks.1.resnets.0.norm2.bias torch.Size([640])\n",
      "down_blocks.1.resnets.0.conv2.weight torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.0.conv2.bias torch.Size([640])\n",
      "down_blocks.1.resnets.0.conv_shortcut.weight torch.Size([640, 320, 1, 1])\n",
      "down_blocks.1.resnets.0.conv_shortcut.bias torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm1.weight torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm1.bias torch.Size([640])\n",
      "down_blocks.1.resnets.1.conv1.weight torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.1.conv1.bias torch.Size([640])\n",
      "down_blocks.1.resnets.1.time_emb_proj.weight torch.Size([640, 1280])\n",
      "down_blocks.1.resnets.1.time_emb_proj.bias torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm2.weight torch.Size([640])\n",
      "down_blocks.1.resnets.1.norm2.bias torch.Size([640])\n",
      "down_blocks.1.resnets.1.conv2.weight torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.resnets.1.conv2.bias torch.Size([640])\n",
      "down_blocks.1.downsamplers.0.conv.weight torch.Size([640, 640, 3, 3])\n",
      "down_blocks.1.downsamplers.0.conv.bias torch.Size([640])\n",
      "down_blocks.2.attentions.0.norm.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.0.norm.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.0.proj_in.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.0.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.0.proj_out.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.norm.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.1.norm.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.1.proj_in.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "down_blocks.2.attentions.1.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "down_blocks.2.attentions.1.proj_out.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.0.norm1.weight torch.Size([640])\n",
      "down_blocks.2.resnets.0.norm1.bias torch.Size([640])\n",
      "down_blocks.2.resnets.0.conv1.weight torch.Size([1280, 640, 3, 3])\n",
      "down_blocks.2.resnets.0.conv1.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.0.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.resnets.0.time_emb_proj.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.0.norm2.weight torch.Size([1280])\n",
      "down_blocks.2.resnets.0.norm2.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.0.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.0.conv2.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.0.conv_shortcut.weight torch.Size([1280, 640, 1, 1])\n",
      "down_blocks.2.resnets.0.conv_shortcut.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm1.weight torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm1.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.1.conv1.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.1.conv1.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.1.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "down_blocks.2.resnets.1.time_emb_proj.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm2.weight torch.Size([1280])\n",
      "down_blocks.2.resnets.1.norm2.bias torch.Size([1280])\n",
      "down_blocks.2.resnets.1.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.resnets.1.conv2.bias torch.Size([1280])\n",
      "down_blocks.2.downsamplers.0.conv.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.2.downsamplers.0.conv.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm1.weight torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm1.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.0.conv1.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.0.conv1.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.0.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "down_blocks.3.resnets.0.time_emb_proj.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm2.weight torch.Size([1280])\n",
      "down_blocks.3.resnets.0.norm2.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.0.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.0.conv2.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm1.weight torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm1.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.1.conv1.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.1.conv1.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.1.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "down_blocks.3.resnets.1.time_emb_proj.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm2.weight torch.Size([1280])\n",
      "down_blocks.3.resnets.1.norm2.bias torch.Size([1280])\n",
      "down_blocks.3.resnets.1.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "down_blocks.3.resnets.1.conv2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.0.norm1.weight torch.Size([2560])\n",
      "up_blocks.0.resnets.0.norm1.bias torch.Size([2560])\n",
      "up_blocks.0.resnets.0.conv1.weight torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.0.conv1.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.0.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.0.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.0.norm2.weight torch.Size([1280])\n",
      "up_blocks.0.resnets.0.norm2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.0.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.0.conv2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.0.conv_shortcut.weight torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.0.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.1.norm1.weight torch.Size([2560])\n",
      "up_blocks.0.resnets.1.norm1.bias torch.Size([2560])\n",
      "up_blocks.0.resnets.1.conv1.weight torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.1.conv1.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.1.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.1.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.1.norm2.weight torch.Size([1280])\n",
      "up_blocks.0.resnets.1.norm2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.1.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.1.conv2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.1.conv_shortcut.weight torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.1.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.2.norm1.weight torch.Size([2560])\n",
      "up_blocks.0.resnets.2.norm1.bias torch.Size([2560])\n",
      "up_blocks.0.resnets.2.conv1.weight torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.0.resnets.2.conv1.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.2.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.0.resnets.2.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.2.norm2.weight torch.Size([1280])\n",
      "up_blocks.0.resnets.2.norm2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.2.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.resnets.2.conv2.bias torch.Size([1280])\n",
      "up_blocks.0.resnets.2.conv_shortcut.weight torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.0.resnets.2.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.0.upsamplers.0.conv.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.0.upsamplers.0.conv.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.norm.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.0.norm.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.0.proj_in.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.0.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.0.proj_out.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.norm.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.1.norm.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.1.proj_in.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.1.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.1.proj_out.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.norm.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.2.norm.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.2.proj_in.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "up_blocks.1.attentions.2.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "up_blocks.1.attentions.2.proj_out.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.0.norm1.weight torch.Size([2560])\n",
      "up_blocks.1.resnets.0.norm1.bias torch.Size([2560])\n",
      "up_blocks.1.resnets.0.conv1.weight torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.1.resnets.0.conv1.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.0.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.resnets.0.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.0.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.resnets.0.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.0.conv2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.0.conv_shortcut.weight torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.1.resnets.0.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.1.norm1.weight torch.Size([2560])\n",
      "up_blocks.1.resnets.1.norm1.bias torch.Size([2560])\n",
      "up_blocks.1.resnets.1.conv1.weight torch.Size([1280, 2560, 3, 3])\n",
      "up_blocks.1.resnets.1.conv1.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.1.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.resnets.1.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.1.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.resnets.1.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.1.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.1.conv2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.1.conv_shortcut.weight torch.Size([1280, 2560, 1, 1])\n",
      "up_blocks.1.resnets.1.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.2.norm1.weight torch.Size([1920])\n",
      "up_blocks.1.resnets.2.norm1.bias torch.Size([1920])\n",
      "up_blocks.1.resnets.2.conv1.weight torch.Size([1280, 1920, 3, 3])\n",
      "up_blocks.1.resnets.2.conv1.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.2.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "up_blocks.1.resnets.2.time_emb_proj.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.2.norm2.weight torch.Size([1280])\n",
      "up_blocks.1.resnets.2.norm2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.2.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.resnets.2.conv2.bias torch.Size([1280])\n",
      "up_blocks.1.resnets.2.conv_shortcut.weight torch.Size([1280, 1920, 1, 1])\n",
      "up_blocks.1.resnets.2.conv_shortcut.bias torch.Size([1280])\n",
      "up_blocks.1.upsamplers.0.conv.weight torch.Size([1280, 1280, 3, 3])\n",
      "up_blocks.1.upsamplers.0.conv.bias torch.Size([1280])\n",
      "up_blocks.2.attentions.0.norm.weight torch.Size([640])\n",
      "up_blocks.2.attentions.0.norm.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.proj_in.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.0.proj_in.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([5120])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.0.proj_out.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.0.proj_out.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.norm.weight torch.Size([640])\n",
      "up_blocks.2.attentions.1.norm.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.proj_in.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.1.proj_in.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([5120])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.1.proj_out.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.1.proj_out.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.norm.weight torch.Size([640])\n",
      "up_blocks.2.attentions.2.norm.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.proj_in.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.2.proj_in.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([640, 768])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.Size([640, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.Size([5120, 640])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.Size([5120])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight torch.Size([640, 2560])\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias torch.Size([640])\n",
      "up_blocks.2.attentions.2.proj_out.weight torch.Size([640, 640, 1, 1])\n",
      "up_blocks.2.attentions.2.proj_out.bias torch.Size([640])\n",
      "up_blocks.2.resnets.0.norm1.weight torch.Size([1920])\n",
      "up_blocks.2.resnets.0.norm1.bias torch.Size([1920])\n",
      "up_blocks.2.resnets.0.conv1.weight torch.Size([640, 1920, 3, 3])\n",
      "up_blocks.2.resnets.0.conv1.bias torch.Size([640])\n",
      "up_blocks.2.resnets.0.time_emb_proj.weight torch.Size([640, 1280])\n",
      "up_blocks.2.resnets.0.time_emb_proj.bias torch.Size([640])\n",
      "up_blocks.2.resnets.0.norm2.weight torch.Size([640])\n",
      "up_blocks.2.resnets.0.norm2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv2.weight torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.0.conv2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.0.conv_shortcut.weight torch.Size([640, 1920, 1, 1])\n",
      "up_blocks.2.resnets.0.conv_shortcut.bias torch.Size([640])\n",
      "up_blocks.2.resnets.1.norm1.weight torch.Size([1280])\n",
      "up_blocks.2.resnets.1.norm1.bias torch.Size([1280])\n",
      "up_blocks.2.resnets.1.conv1.weight torch.Size([640, 1280, 3, 3])\n",
      "up_blocks.2.resnets.1.conv1.bias torch.Size([640])\n",
      "up_blocks.2.resnets.1.time_emb_proj.weight torch.Size([640, 1280])\n",
      "up_blocks.2.resnets.1.time_emb_proj.bias torch.Size([640])\n",
      "up_blocks.2.resnets.1.norm2.weight torch.Size([640])\n",
      "up_blocks.2.resnets.1.norm2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.1.conv2.weight torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.1.conv2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.1.conv_shortcut.weight torch.Size([640, 1280, 1, 1])\n",
      "up_blocks.2.resnets.1.conv_shortcut.bias torch.Size([640])\n",
      "up_blocks.2.resnets.2.norm1.weight torch.Size([960])\n",
      "up_blocks.2.resnets.2.norm1.bias torch.Size([960])\n",
      "up_blocks.2.resnets.2.conv1.weight torch.Size([640, 960, 3, 3])\n",
      "up_blocks.2.resnets.2.conv1.bias torch.Size([640])\n",
      "up_blocks.2.resnets.2.time_emb_proj.weight torch.Size([640, 1280])\n",
      "up_blocks.2.resnets.2.time_emb_proj.bias torch.Size([640])\n",
      "up_blocks.2.resnets.2.norm2.weight torch.Size([640])\n",
      "up_blocks.2.resnets.2.norm2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.2.conv2.weight torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.resnets.2.conv2.bias torch.Size([640])\n",
      "up_blocks.2.resnets.2.conv_shortcut.weight torch.Size([640, 960, 1, 1])\n",
      "up_blocks.2.resnets.2.conv_shortcut.bias torch.Size([640])\n",
      "up_blocks.2.upsamplers.0.conv.weight torch.Size([640, 640, 3, 3])\n",
      "up_blocks.2.upsamplers.0.conv.bias torch.Size([640])\n",
      "up_blocks.3.attentions.0.norm.weight torch.Size([320])\n",
      "up_blocks.3.attentions.0.norm.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.proj_in.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.0.proj_in.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([2560])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.0.proj_out.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.0.proj_out.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.norm.weight torch.Size([320])\n",
      "up_blocks.3.attentions.1.norm.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.proj_in.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.1.proj_in.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.Size([2560])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.1.proj_out.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.1.proj_out.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.norm.weight torch.Size([320])\n",
      "up_blocks.3.attentions.2.norm.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.proj_in.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.2.proj_in.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.Size([320, 768])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.Size([320, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.Size([2560, 320])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.Size([2560])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight torch.Size([320, 1280])\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias torch.Size([320])\n",
      "up_blocks.3.attentions.2.proj_out.weight torch.Size([320, 320, 1, 1])\n",
      "up_blocks.3.attentions.2.proj_out.bias torch.Size([320])\n",
      "up_blocks.3.resnets.0.norm1.weight torch.Size([960])\n",
      "up_blocks.3.resnets.0.norm1.bias torch.Size([960])\n",
      "up_blocks.3.resnets.0.conv1.weight torch.Size([320, 960, 3, 3])\n",
      "up_blocks.3.resnets.0.conv1.bias torch.Size([320])\n",
      "up_blocks.3.resnets.0.time_emb_proj.weight torch.Size([320, 1280])\n",
      "up_blocks.3.resnets.0.time_emb_proj.bias torch.Size([320])\n",
      "up_blocks.3.resnets.0.norm2.weight torch.Size([320])\n",
      "up_blocks.3.resnets.0.norm2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv2.weight torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.0.conv2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.0.conv_shortcut.weight torch.Size([320, 960, 1, 1])\n",
      "up_blocks.3.resnets.0.conv_shortcut.bias torch.Size([320])\n",
      "up_blocks.3.resnets.1.norm1.weight torch.Size([640])\n",
      "up_blocks.3.resnets.1.norm1.bias torch.Size([640])\n",
      "up_blocks.3.resnets.1.conv1.weight torch.Size([320, 640, 3, 3])\n",
      "up_blocks.3.resnets.1.conv1.bias torch.Size([320])\n",
      "up_blocks.3.resnets.1.time_emb_proj.weight torch.Size([320, 1280])\n",
      "up_blocks.3.resnets.1.time_emb_proj.bias torch.Size([320])\n",
      "up_blocks.3.resnets.1.norm2.weight torch.Size([320])\n",
      "up_blocks.3.resnets.1.norm2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.1.conv2.weight torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.1.conv2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.1.conv_shortcut.weight torch.Size([320, 640, 1, 1])\n",
      "up_blocks.3.resnets.1.conv_shortcut.bias torch.Size([320])\n",
      "up_blocks.3.resnets.2.norm1.weight torch.Size([640])\n",
      "up_blocks.3.resnets.2.norm1.bias torch.Size([640])\n",
      "up_blocks.3.resnets.2.conv1.weight torch.Size([320, 640, 3, 3])\n",
      "up_blocks.3.resnets.2.conv1.bias torch.Size([320])\n",
      "up_blocks.3.resnets.2.time_emb_proj.weight torch.Size([320, 1280])\n",
      "up_blocks.3.resnets.2.time_emb_proj.bias torch.Size([320])\n",
      "up_blocks.3.resnets.2.norm2.weight torch.Size([320])\n",
      "up_blocks.3.resnets.2.norm2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.2.conv2.weight torch.Size([320, 320, 3, 3])\n",
      "up_blocks.3.resnets.2.conv2.bias torch.Size([320])\n",
      "up_blocks.3.resnets.2.conv_shortcut.weight torch.Size([320, 640, 1, 1])\n",
      "up_blocks.3.resnets.2.conv_shortcut.bias torch.Size([320])\n",
      "mid_block.attentions.0.norm.weight torch.Size([1280])\n",
      "mid_block.attentions.0.norm.bias torch.Size([1280])\n",
      "mid_block.attentions.0.proj_in.weight torch.Size([1280, 1280, 1, 1])\n",
      "mid_block.attentions.0.proj_in.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.Size([1280, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.Size([1280, 768])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.Size([1280, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias torch.Size([1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.Size([10240, 1280])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.Size([10240])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight torch.Size([1280, 5120])\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias torch.Size([1280])\n",
      "mid_block.attentions.0.proj_out.weight torch.Size([1280, 1280, 1, 1])\n",
      "mid_block.attentions.0.proj_out.bias torch.Size([1280])\n",
      "mid_block.resnets.0.norm1.weight torch.Size([1280])\n",
      "mid_block.resnets.0.norm1.bias torch.Size([1280])\n",
      "mid_block.resnets.0.conv1.weight torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.0.conv1.bias torch.Size([1280])\n",
      "mid_block.resnets.0.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "mid_block.resnets.0.time_emb_proj.bias torch.Size([1280])\n",
      "mid_block.resnets.0.norm2.weight torch.Size([1280])\n",
      "mid_block.resnets.0.norm2.bias torch.Size([1280])\n",
      "mid_block.resnets.0.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.0.conv2.bias torch.Size([1280])\n",
      "mid_block.resnets.1.norm1.weight torch.Size([1280])\n",
      "mid_block.resnets.1.norm1.bias torch.Size([1280])\n",
      "mid_block.resnets.1.conv1.weight torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.1.conv1.bias torch.Size([1280])\n",
      "mid_block.resnets.1.time_emb_proj.weight torch.Size([1280, 1280])\n",
      "mid_block.resnets.1.time_emb_proj.bias torch.Size([1280])\n",
      "mid_block.resnets.1.norm2.weight torch.Size([1280])\n",
      "mid_block.resnets.1.norm2.bias torch.Size([1280])\n",
      "mid_block.resnets.1.conv2.weight torch.Size([1280, 1280, 3, 3])\n",
      "mid_block.resnets.1.conv2.bias torch.Size([1280])\n",
      "conv_norm_out.weight torch.Size([320])\n",
      "conv_norm_out.bias torch.Size([320])\n",
      "conv_out.weight torch.Size([4, 320, 3, 3])\n",
      "conv_out.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "unet_sd = unet.state_dict()\n",
    "for key, value in unet_sd.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision\n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.15.2+cu118\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 4),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['num_attention_heads',\n",
       "              'encoder_hid_dim',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'mid_block_type',\n",
       "              'time_embedding_dim',\n",
       "              'timestep_post_act',\n",
       "              'resnet_skip_time_act',\n",
       "              'time_embedding_act_fn',\n",
       "              'attention_type',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'num_class_embeds',\n",
       "              'upcast_attention',\n",
       "              'addition_embed_type',\n",
       "              'cross_attention_norm',\n",
       "              'class_embed_type',\n",
       "              'dual_cross_attention',\n",
       "              'transformer_layers_per_block',\n",
       "              'resnet_time_scale_shift',\n",
       "              'dropout',\n",
       "              'conv_in_kernel',\n",
       "              'conv_out_kernel',\n",
       "              'reverse_transformer_layers_per_block',\n",
       "              'encoder_hid_dim_type',\n",
       "              'resnet_out_scale_factor',\n",
       "              'addition_time_embed_dim',\n",
       "              'only_cross_attention',\n",
       "              'time_embedding_type',\n",
       "              'time_cond_proj_dim',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embeddings_concat',\n",
       "              'use_linear_projection']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.6.0'),\n",
       "            ('_name_or_path', 'runwayml/stable-diffusion-v1-5')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577640>,\n",
       " 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577cd0>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5420>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5ab0>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7700>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7d90>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26254e0>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2625b70>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627790>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627e20>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659570>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659c00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c8a00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c9090>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26caa10>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26cb0a0>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fca00>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fd090>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fecb0>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26ff340>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2730ca0>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2731330>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2732c50>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27332e0>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2764f40>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27653c0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2765fc0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766080>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27667a0>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766860>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690490>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690b20>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixEncoder\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import MyDataset, collate_fn\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--pretrained_ip_adapter_path\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     help=\"Path to pretrained ip adapter model. If not specified weights are initialized randomly.\",\n",
    "    # )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\pic\",\n",
    "        required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-ip_adapter\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    image_proj_model = VPmatrixEncoder()\n",
    "\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    ip_adapter = IPAdapter(unet, image_proj_model, adapter_modules, args.pretrained_ip_adapter_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(ip_adapter.image_proj_model.parameters(),  ip_adapter.adapter_modules.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MyDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    ip_adapter, optimizer, train_dataloader = accelerator.prepare(ip_adapter, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(ip_adapter):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"images\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    image_embeds = image_encoder(batch[\"clip_images\"].to(accelerator.device, dtype=weight_dtype)).image_embeds\n",
    "                image_embeds_ = []\n",
    "                for image_embed, drop_image_embed in zip(image_embeds, batch[\"drop_image_embeds\"]):\n",
    "                    if drop_image_embed == 1:\n",
    "                        image_embeds_.append(torch.zeros_like(image_embed))\n",
    "                    else:\n",
    "                        image_embeds_.append(image_embed)\n",
    "                image_embeds = torch.stack(image_embeds_)\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                \n",
    "                noise_pred = ip_adapter(noisy_latents, timesteps, encoder_hidden_states, image_embeds)\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"checkpoint-50000/pytorch_model.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "VPmatrixEncoder_sd = {}\n",
    "atten_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"VPmatrixEncoder\"):\n",
    "        VPmatrixEncoder_sd[k.replace(\"VPmatrixEncoder.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"atten_modules\"):\n",
    "        atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"VPmatrixEncoder\": VPmatrixEncoder_sd, \"atten_modules\": atten_sd}, \"posectrl.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- unet  attention\n",
    "- main\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
