{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet 2d condition model\n",
    "\n",
    "[unet](https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "unet_sd = unet.state_dict()2\n",
    "# for key, value in unet_sd.items():\n",
    "#     print(key, value.shape)\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    if cross_attention_dim is None:\n",
    "        pass\n",
    "    else:\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision\n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.15.2+cu118\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 4),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['num_attention_heads',\n",
       "              'encoder_hid_dim',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'mid_block_type',\n",
       "              'time_embedding_dim',\n",
       "              'timestep_post_act',\n",
       "              'resnet_skip_time_act',\n",
       "              'time_embedding_act_fn',\n",
       "              'attention_type',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'num_class_embeds',\n",
       "              'upcast_attention',\n",
       "              'addition_embed_type',\n",
       "              'cross_attention_norm',\n",
       "              'class_embed_type',\n",
       "              'dual_cross_attention',\n",
       "              'transformer_layers_per_block',\n",
       "              'resnet_time_scale_shift',\n",
       "              'dropout',\n",
       "              'conv_in_kernel',\n",
       "              'conv_out_kernel',\n",
       "              'reverse_transformer_layers_per_block',\n",
       "              'encoder_hid_dim_type',\n",
       "              'resnet_out_scale_factor',\n",
       "              'addition_time_embed_dim',\n",
       "              'only_cross_attention',\n",
       "              'time_embedding_type',\n",
       "              'time_cond_proj_dim',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embeddings_concat',\n",
       "              'use_linear_projection']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.6.0'),\n",
       "            ('_name_or_path', 'runwayml/stable-diffusion-v1-5')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577640>,\n",
       " 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577cd0>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5420>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5ab0>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7700>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7d90>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26254e0>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2625b70>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627790>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627e20>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659570>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659c00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c8a00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c9090>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26caa10>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26cb0a0>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fca00>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fd090>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fecb0>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26ff340>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2730ca0>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2731330>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2732c50>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27332e0>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2764f40>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27653c0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2765fc0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766080>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27667a0>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766860>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690490>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690b20>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixEncoder, VPmatrixPoints\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import MyDataset, load_base_points\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_pose_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to pretrained  posectrl model. If not specified weights are initialized randomly.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--base_point_path\",\n",
    "        type=str,\n",
    "        default=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt',\n",
    "        help='Path to base model points'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\pic\",\n",
    "        required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-pose_ctrl\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "class posectrl(nn.Module):\n",
    "    def __init__(self, unet, vpmatrix_points, atten_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.vpmatrix_points = vpmatrix_points\n",
    "        self.atten_modules = atten_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, V_matrix, P_matrix):\n",
    "        point_tokens = self.vpmatrix_points(V_matrix, P_matrix)\n",
    "        \"\"\" 修改:防止之后要加text \"\"\"\n",
    "        if encoder_hidden_states:\n",
    "            encoder_hidden_states = torch.cat([encoder_hidden_states, point_tokens], dim=1)\n",
    "        else:\n",
    "            encoder_hidden_states=point_tokens\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        orig_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.vpmatrix_points.load_state_dict(state_dict[\"vpmatrix_points\"], strict=True)\n",
    "        self.atten_modules.load_state_dict(state_dict[\"atten_modules\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        new_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_VPmatrix_sum != new_VPmatrix_sum, \"Weights of VPmatrixEncoder did not change!\"\n",
    "        assert orig_atten_sum != new_atten_sum, \"Weights of atten_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    raw_base_points=load_base_points(args.base_point_path)  \n",
    "    vpmatrix_points_sd = VPmatrixEncoder(raw_base_points)\n",
    "\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    atten_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    pose_ctrl = posectrl(unet, vpmatrix_points_sd, atten_modules, args.pretrained_pose_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(pose_ctrl.vpmatrix_points_sd.parameters(),  pose_ctrl.atten_modules.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MyDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(pose_ctrl):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"image\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                if \"text_input_ids\" in batch:\n",
    "                    with torch.no_grad():\n",
    "                        encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                else:\n",
    "                    encoder_hidden_states=None\n",
    "                \n",
    "                noise_pred = pose_ctrl(noisy_latents, timesteps, encoder_hidden_states, batch['view_matrix'], batch['projection_matrix'])\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "里面结构没改\n",
    "\n",
    "应该有两个输入：图像和VP\n",
    "\n",
    "VP： VPmatrixEncoder -> [77,77]\n",
    "\n",
    "image: \n",
    "- :vae ->latent\n",
    "- :resampler? 写一个 vit或者别的网络\n",
    "- 那么训练参数会变成三个\n",
    "\n",
    "\n",
    "# TODO\n",
    "- 1. visEncoder.py\n",
    "- 2. attention_processor.py： 加逻辑\n",
    "- 3. posectrl.py: 加参数和逻辑\n",
    "- 4. main.py：加参数\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"checkpoint-50000/pytorch_model.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "VPmatrixEncoder_sd = {}\n",
    "atten_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"VPmatrixEncoder\"):\n",
    "        VPmatrixEncoder_sd[k.replace(\"VPmatrixEncoder.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"atten_modules\"):\n",
    "        atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"VPmatrixEncoder\": VPmatrixEncoder_sd, \"atten_modules\": atten_sd}, \"posectrl.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 位置矩阵\n",
    "2. 本地的坐标：可以和vp矩阵相乘，数学意义，M矩阵，\n",
    "3. 多样性一点：图的特征，加上正面原图随便的特征。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 反向：训练什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO NEW Version 1\n",
    "\n",
    "- inference\n",
    "- VP矩阵不需要处理了\n",
    "- BasePoints: [2000,4,4] @ [4,4] -> <77 768>（这个是text attention之后的结果，不知道图片他们都是怎么做的）可能可以换个大小，可学习的部分直接写出来更换就行。\n",
    "- 好像流程就没啥问题了\n",
    "- 每个要改的地方加上\"修改\",不然找不到忘记了.\n",
    "\n",
    "# TODO NEW Version 2\n",
    "   现在的逻辑是： vp矩阵[4,4], 顶点是[13860,4] (77x180), ->[13840,4] reshape [77,768]\n",
    "-  要改：\n",
    "   \n",
    "~~- base_load~~\n",
    "\n",
    "~~- pose_adaptor~~\n",
    "\n",
    "   ~~- posectrl traning~~ \n",
    "   - 和 posectrl inference   \n",
    " \n",
    "   ~~- attention_pocessor~~ 和之前没区别\n",
    "\n",
    "   ~~- train main~~\n",
    "\n",
    "   ~~- 跑通~~\n",
    "\n",
    "# TODO NEW Version 3\n",
    "加了个参考图， 这个图attention 加上\n",
    "\n",
    "~~- dataset 1024 resize~~, 可以不加数量限制\n",
    "\n",
    "~~- image sampler~~\n",
    "\n",
    "~~- posectrl train main~~\n",
    "\n",
    "~~- attention~~\n",
    "\n",
    "- posectrl.py\n",
    "- inference\n",
    "- validaton\n",
    "- train 得到weights\n",
    "- inference\n",
    "\n",
    "# Questions\n",
    "~~1. 需不要把好坏prompt设置成~~\n",
    " \n",
    "python \n",
    "```\n",
    "if prompt is None:\n",
    "    prompt = \"best quality, high quality\"\n",
    "if negative_prompt is None:\n",
    "    negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "```\n",
    "\n",
    "不用应该\n",
    "\n",
    "- 2. point-e好像是生成3d底模的东西,不知道有没有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch feature shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 选择 CLIP 预训练模型\n",
    "model_name = \"openai/clip-vit-base-patch16\"  # 也可以换成 \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 读取并预处理图片\n",
    "image_path = r\"F:\\Projects\\diffusers\\ProgramData\\sample_new\\NPC_Avatar_Girl_Sword_Ayaka\\feature.png\"  # 替换成你的图片路径\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")  # 预处理\n",
    "image_tensor = inputs[\"pixel_values\"]  # 获取输入张量，形状 (1, 3, 224, 224)\n",
    "\n",
    "# 3. 获取所有 patch 的特征\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.vision_model(image_tensor)  # 获取所有 Transformer 层的输出\n",
    "    patch_features = vision_outputs.last_hidden_state  # 形状: (B, X+1, 768)\n",
    "\n",
    "# 4. 移除 CLS token（第一个 token）\n",
    "patch_features = patch_features[:, 1:, :]  # (B, X, 768)\n",
    "\n",
    "print(\"Patch feature shape:\", patch_features.shape)  # 目标形状: (B, X, 768)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
