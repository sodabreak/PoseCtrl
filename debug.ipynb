{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet 2d condition model\n",
    "\n",
    "[unet](https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "unet_sd = unet.state_dict()\n",
    "# for key, value in unet_sd.items():\n",
    "#     print(key, value.shape)\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    if cross_attention_dim is None:\n",
    "        pass\n",
    "    else:\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision\n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.15.2+cu118\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 4),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['num_attention_heads',\n",
       "              'encoder_hid_dim',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'mid_block_type',\n",
       "              'time_embedding_dim',\n",
       "              'timestep_post_act',\n",
       "              'resnet_skip_time_act',\n",
       "              'time_embedding_act_fn',\n",
       "              'attention_type',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'num_class_embeds',\n",
       "              'upcast_attention',\n",
       "              'addition_embed_type',\n",
       "              'cross_attention_norm',\n",
       "              'class_embed_type',\n",
       "              'dual_cross_attention',\n",
       "              'transformer_layers_per_block',\n",
       "              'resnet_time_scale_shift',\n",
       "              'dropout',\n",
       "              'conv_in_kernel',\n",
       "              'conv_out_kernel',\n",
       "              'reverse_transformer_layers_per_block',\n",
       "              'encoder_hid_dim_type',\n",
       "              'resnet_out_scale_factor',\n",
       "              'addition_time_embed_dim',\n",
       "              'only_cross_attention',\n",
       "              'time_embedding_type',\n",
       "              'time_cond_proj_dim',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embeddings_concat',\n",
       "              'use_linear_projection']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.6.0'),\n",
       "            ('_name_or_path', 'runwayml/stable-diffusion-v1-5')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577640>,\n",
       " 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577cd0>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5420>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5ab0>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7700>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7d90>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26254e0>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2625b70>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627790>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627e20>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659570>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659c00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c8a00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c9090>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26caa10>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26cb0a0>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fca00>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fd090>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fecb0>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26ff340>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2730ca0>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2731330>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2732c50>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27332e0>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2764f40>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27653c0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2765fc0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766080>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27667a0>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766860>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690490>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690b20>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixEncoder, VPmatrixPoints\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import MyDataset, load_base_points\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_pose_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to pretrained  posectrl model. If not specified weights are initialized randomly.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--base_point_path\",\n",
    "        type=str,\n",
    "        default=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt',\n",
    "        help='Path to base model points'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\pic\",\n",
    "        required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-pose_ctrl\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "class posectrl(nn.Module):\n",
    "    def __init__(self, unet, vpmatrix_points, atten_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.vpmatrix_points = vpmatrix_points\n",
    "        self.atten_modules = atten_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, V_matrix, P_matrix):\n",
    "        point_tokens = self.vpmatrix_points(V_matrix, P_matrix)\n",
    "        \"\"\" 修改:防止之后要加text \"\"\"\n",
    "        if encoder_hidden_states:\n",
    "            encoder_hidden_states = torch.cat([encoder_hidden_states, point_tokens], dim=1)\n",
    "        else:\n",
    "            encoder_hidden_states=point_tokens\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        orig_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.vpmatrix_points.load_state_dict(state_dict[\"vpmatrix_points\"], strict=True)\n",
    "        self.atten_modules.load_state_dict(state_dict[\"atten_modules\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        new_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_VPmatrix_sum != new_VPmatrix_sum, \"Weights of VPmatrixEncoder did not change!\"\n",
    "        assert orig_atten_sum != new_atten_sum, \"Weights of atten_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    raw_base_points=load_base_points(args.base_point_path)  \n",
    "    vpmatrix_points_sd = VPmatrixEncoder(raw_base_points)\n",
    "\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    atten_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    pose_ctrl = posectrl(unet, vpmatrix_points_sd, atten_modules, args.pretrained_pose_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(pose_ctrl.vpmatrix_points_sd.parameters(),  pose_ctrl.atten_modules.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MyDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(pose_ctrl):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"image\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                if \"text_input_ids\" in batch:\n",
    "                    with torch.no_grad():\n",
    "                        encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                else:\n",
    "                    encoder_hidden_states=None\n",
    "                \n",
    "                noise_pred = pose_ctrl(noisy_latents, timesteps, encoder_hidden_states, batch['view_matrix'], batch['projection_matrix'])\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "里面结构没改\n",
    "\n",
    "应该有两个输入：图像和VP\n",
    "\n",
    "VP： VPmatrixEncoder -> [77,77]\n",
    "\n",
    "image: \n",
    "- :vae ->latent\n",
    "- :resampler? 写一个 vit或者别的网络\n",
    "- 那么训练参数会变成三个\n",
    "\n",
    "\n",
    "# TODO\n",
    "- 1. visEncoder.py\n",
    "- 2. attention_processor.py： 加逻辑\n",
    "- 3. posectrl.py: 加参数和逻辑\n",
    "- 4. main.py：加参数\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"checkpoint-50000/pytorch_model.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "VPmatrixEncoder_sd = {}\n",
    "atten_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"VPmatrixEncoder\"):\n",
    "        VPmatrixEncoder_sd[k.replace(\"VPmatrixEncoder.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"atten_modules\"):\n",
    "        atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"VPmatrixEncoder\": VPmatrixEncoder_sd, \"atten_modules\": atten_sd}, \"posectrl.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 位置矩阵\n",
    "2. 本地的坐标：可以和vp矩阵相乘，数学意义，M矩阵，\n",
    "3. 多样性一点：图的特征，加上正面原图随便的特征。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 反向：训练什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO NEW Version 1\n",
    "\n",
    "- inference\n",
    "- VP矩阵不需要处理了\n",
    "- BasePoints: [2000,4,4] @ [4,4] -> <77 768>（这个是text attention之后的结果，不知道图片他们都是怎么做的）可能可以换个大小，可学习的部分直接写出来更换就行。\n",
    "- 好像流程就没啥问题了\n",
    "- 每个要改的地方加上\"修改\",不然找不到忘记了.\n",
    "\n",
    "# TODO NEW Version 2\n",
    "   现在的逻辑是： vp矩阵[4,4], 顶点是[13860,4] (77x180), ->[13840,4] reshape [77,768]\n",
    "-  要改：\n",
    "   \n",
    "~~- base_load~~\n",
    "\n",
    "~~- pose_adaptor~~\n",
    "\n",
    "   ~~- posectrl traning~~ \n",
    "   - 和 posectrl inference   \n",
    " \n",
    "   ~~- attention_pocessor~~ 和之前没区别\n",
    "\n",
    "   ~~- train main~~\n",
    "\n",
    "   ~~- 跑通~~\n",
    "\n",
    "# TODO NEW Version 3\n",
    "加了个参考图， 这个图attention 加上\n",
    "\n",
    "~~- dataset 1024 resize~~, 可以不加数量限制\n",
    "\n",
    "~~- image sampler~~\n",
    "\n",
    "~~- posectrl train main~~\n",
    "\n",
    "~~- attention~~\n",
    "\n",
    "- posectrl.py\n",
    "- inference\n",
    "- validaton\n",
    "- train 得到weights\n",
    "- inference\n",
    "\n",
    "# Questions\n",
    "~~1. 需不要把好坏prompt设置成~~\n",
    " \n",
    "python \n",
    "```\n",
    "if prompt is None:\n",
    "    prompt = \"best quality, high quality\"\n",
    "if negative_prompt is None:\n",
    "    negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "```\n",
    "\n",
    "不用应该\n",
    "\n",
    "- 2. point-e好像是生成3d底模的东西,不知道有没有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch feature shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 选择 CLIP 预训练模型\n",
    "model_name = \"openai/clip-vit-base-patch16\"  # 也可以换成 \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 读取并预处理图片\n",
    "image_path = r\"F:\\Projects\\diffusers\\ProgramData\\sample_new\\NPC_Avatar_Girl_Sword_Ayaka\\feature.png\"  # 替换成你的图片路径\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")  # 预处理\n",
    "image_tensor = inputs[\"pixel_values\"]  # 获取输入张量，形状 (1, 3, 224, 224)\n",
    "\n",
    "# 3. 获取所有 patch 的特征\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.vision_model(image_tensor)  # 获取所有 Transformer 层的输出\n",
    "    patch_features = vision_outputs.last_hidden_state  # 形状: (B, X+1, 768)\n",
    "\n",
    "# 4. 移除 CLS token（第一个 token）\n",
    "patch_features = patch_features[:, 1:, :]  # (B, X, 768)\n",
    "\n",
    "print(\"Patch feature shape:\", patch_features.shape)  # 目标形状: (B, X, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages (0.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x18'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved new checkpoint to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_checkpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdiffusers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPoseCtrl\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msd-pose_ctrl\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mchange_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdiffusers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPoseCtrl\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msd-pose_ctrl\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtransfer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [7], line 5\u001b[0m, in \u001b[0;36mchange_checkpoint\u001b[1;34m(checkpoint_path, new_checkpoint_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchange_checkpoint\u001b[39m(checkpoint_path, new_checkpoint_path):\n\u001b[1;32m----> 5\u001b[0m     sd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     vpmatrix_points_sd \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m     atten_sd \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32md:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\torch\\serialization.py:1033\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1029\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1030\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1033\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x18'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "from pathlib import Path\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    vpmatrix_points_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd={}\n",
    "    for k in sd:\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"vpmatrix_points\"):\n",
    "            vpmatrix_points_sd[k.replace(\"vpmatrix_points.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl.bin\")\n",
    "    print(vpmatrix_points_sd)\n",
    "    print(atten_sd)\n",
    "    print(proj_sd)\n",
    "    for name in sd['state'].keys():\n",
    "        print(name)\n",
    "    torch.save({\"vpmatrix_points\": vpmatrix_points_sd, \"atten_modules\": atten_sd, \"image_proj_model\": proj_sd}, new_checkpoint_path)\n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "ckpt = r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\model.safetensors\"\n",
    "\n",
    "change_checkpoint(ckpt, r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([320, 768]) 1\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([320, 768]) 1\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([320, 768]) 3\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([320, 768]) 3\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([640, 768]) 5\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([640, 768]) 5\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([640, 768]) 7\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([640, 768]) 7\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 9\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 9\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 11\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 11\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 13\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 13\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 15\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 15\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 17\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 17\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([1280, 768]) 19\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([1280, 768]) 19\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([640, 768]) 21\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([640, 768]) 21\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([640, 768]) 23\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([640, 768]) 23\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([640, 768]) 25\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([640, 768]) 25\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([320, 768]) 27\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([320, 768]) 27\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([320, 768]) 29\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([320, 768]) 29\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight torch.Size([320, 768]) 31\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight torch.Size([320, 768]) 31\n",
      "{'31.to_k_ip.weight': tensor([[-0.0065,  0.0132, -0.0028,  ..., -0.0033,  0.0308,  0.0227],\n",
      "        [ 0.0080, -0.0480,  0.0373,  ..., -0.0060,  0.0455,  0.0155],\n",
      "        [-0.0357,  0.0118, -0.0137,  ...,  0.0052, -0.0451, -0.0260],\n",
      "        ...,\n",
      "        [-0.0271,  0.0085,  0.0243,  ..., -0.0136, -0.0057, -0.0120],\n",
      "        [ 0.0143,  0.0092, -0.0138,  ...,  0.0592,  0.0064, -0.0519],\n",
      "        [ 0.0661,  0.0127,  0.0268,  ..., -0.0758, -0.0880,  0.0991]]), '31.to_v_ip.weight': tensor([[ 0.0035,  0.0043,  0.0010,  ..., -0.0058, -0.0055,  0.0105],\n",
      "        [ 0.0067,  0.0251, -0.0053,  ...,  0.0087,  0.0110,  0.0010],\n",
      "        [-0.0129, -0.0031,  0.0133,  ...,  0.0007, -0.0104, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0086, -0.0129,  0.0013,  ...,  0.0094,  0.0074,  0.0191],\n",
      "        [-0.0027, -0.0009,  0.0189,  ..., -0.0010,  0.0096,  0.0106],\n",
      "        [ 0.0004,  0.0368,  0.0139,  ..., -0.0083, -0.0237,  0.0085]]), '29.to_k_ip.weight': tensor([[-0.0142, -0.0629, -0.0299,  ..., -0.0217, -0.0629,  0.0011],\n",
      "        [ 0.0497,  0.1232,  0.0318,  ...,  0.0021,  0.0501, -0.0198],\n",
      "        [-0.0192,  0.0201,  0.0523,  ...,  0.1008,  0.0459,  0.0308],\n",
      "        ...,\n",
      "        [-0.0990,  0.1024,  0.0437,  ..., -0.0269, -0.0546,  0.0282],\n",
      "        [ 0.0030, -0.0839, -0.0550,  ..., -0.0557,  0.0133,  0.0513],\n",
      "        [-0.0254,  0.0016,  0.0420,  ...,  0.0206,  0.0363, -0.0311]]), '29.to_v_ip.weight': tensor([[ 0.0013, -0.0088, -0.0085,  ...,  0.0070, -0.0071, -0.0181],\n",
      "        [ 0.0027, -0.0018, -0.0155,  ...,  0.0015, -0.0025, -0.0149],\n",
      "        [-0.0266,  0.0404, -0.0116,  ...,  0.0030,  0.0060, -0.0103],\n",
      "        ...,\n",
      "        [ 0.0111, -0.0251, -0.0026,  ...,  0.0015,  0.0053,  0.0239],\n",
      "        [-0.0100,  0.0062, -0.0011,  ...,  0.0005, -0.0253,  0.0104],\n",
      "        [-0.0022, -0.0543,  0.0113,  ..., -0.0140, -0.0075, -0.0110]]), '27.to_k_ip.weight': tensor([[-0.0031,  0.0199,  0.0647,  ..., -0.0173, -0.0584, -0.0356],\n",
      "        [-0.0315,  0.0535, -0.0163,  ..., -0.0302, -0.0748,  0.0178],\n",
      "        [ 0.0747, -0.1144, -0.0372,  ...,  0.0854, -0.0565,  0.0869],\n",
      "        ...,\n",
      "        [-0.0361,  0.0031,  0.0121,  ...,  0.0143, -0.0086,  0.0037],\n",
      "        [ 0.0309,  0.0124, -0.0894,  ..., -0.1055,  0.0091, -0.0852],\n",
      "        [ 0.1492, -0.0656, -0.0474,  ..., -0.0083, -0.0582,  0.0551]]), '27.to_v_ip.weight': tensor([[ 0.0163,  0.0160,  0.0021,  ..., -0.0145, -0.0019,  0.0280],\n",
      "        [-0.0102, -0.0209, -0.0065,  ..., -0.0236, -0.0129,  0.0051],\n",
      "        [-0.0157,  0.0280,  0.0036,  ..., -0.0103, -0.0099, -0.0186],\n",
      "        ...,\n",
      "        [ 0.0077, -0.0037, -0.0133,  ..., -0.0052,  0.0191,  0.0090],\n",
      "        [-0.0219,  0.0313,  0.0237,  ..., -0.0121, -0.0183,  0.0014],\n",
      "        [ 0.0031, -0.0056,  0.0184,  ..., -0.0057, -0.0174,  0.0091]]), '25.to_k_ip.weight': tensor([[ 0.0732,  0.0393, -0.0010,  ..., -0.0031, -0.0927,  0.0700],\n",
      "        [ 0.0193, -0.1816, -0.0213,  ...,  0.0214, -0.0043,  0.0260],\n",
      "        [-0.0665, -0.0488, -0.0016,  ...,  0.0619, -0.0117,  0.0256],\n",
      "        ...,\n",
      "        [-0.0493,  0.0203,  0.0108,  ..., -0.0701, -0.0003, -0.0514],\n",
      "        [-0.0092, -0.0170,  0.0254,  ...,  0.0133, -0.0094,  0.1019],\n",
      "        [ 0.0624,  0.0884, -0.0836,  ...,  0.0175,  0.0597, -0.0256]]), '25.to_v_ip.weight': tensor([[ 0.0093,  0.0340, -0.0021,  ...,  0.0338, -0.0004,  0.0497],\n",
      "        [-0.0051, -0.0713,  0.0467,  ..., -0.0184, -0.0141,  0.0166],\n",
      "        [-0.0160, -0.0321,  0.0733,  ..., -0.0211, -0.0107,  0.0591],\n",
      "        ...,\n",
      "        [-0.0473,  0.0328,  0.0782,  ..., -0.0023, -0.0009, -0.0697],\n",
      "        [-0.0069, -0.0474,  0.0818,  ..., -0.0042,  0.0396, -0.0092],\n",
      "        [-0.0117, -0.0240, -0.0042,  ...,  0.0059,  0.0125,  0.0783]]), '23.to_k_ip.weight': tensor([[-0.0382, -0.0471, -0.0337,  ..., -0.0189,  0.0250, -0.0227],\n",
      "        [ 0.0745,  0.0800, -0.0100,  ...,  0.0305,  0.0210, -0.0677],\n",
      "        [ 0.0437,  0.0276,  0.0473,  ...,  0.0425, -0.0102,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0157,  0.0568,  0.0312,  ...,  0.0144,  0.0026,  0.0479],\n",
      "        [ 0.0439,  0.0953,  0.0467,  ...,  0.0247, -0.0261, -0.0019],\n",
      "        [ 0.0440, -0.0157, -0.0961,  ...,  0.0819, -0.0407, -0.0648]]), '23.to_v_ip.weight': tensor([[-0.0067, -0.0321, -0.0221,  ...,  0.0344, -0.0115,  0.0260],\n",
      "        [ 0.0448, -0.0461,  0.0128,  ...,  0.0272, -0.0066,  0.0557],\n",
      "        [-0.0007,  0.0573,  0.0225,  ..., -0.0066, -0.0277, -0.0146],\n",
      "        ...,\n",
      "        [-0.0479,  0.0908, -0.1083,  ...,  0.0395, -0.0514, -0.0775],\n",
      "        [-0.0667,  0.0044,  0.1094,  ...,  0.0143, -0.0117, -0.0109],\n",
      "        [ 0.0627,  0.0769, -0.0594,  ..., -0.0052, -0.0301,  0.0342]]), '21.to_k_ip.weight': tensor([[-0.1159,  0.0307, -0.0336,  ...,  0.0027, -0.0221, -0.0651],\n",
      "        [ 0.0162,  0.0465,  0.0301,  ...,  0.0598, -0.0529, -0.0460],\n",
      "        [-0.0238, -0.0160,  0.0902,  ...,  0.0312,  0.1006,  0.0812],\n",
      "        ...,\n",
      "        [-0.0474,  0.0390,  0.0108,  ...,  0.0214,  0.0172, -0.0410],\n",
      "        [-0.0889, -0.0579,  0.0592,  ...,  0.0641,  0.0424,  0.0022],\n",
      "        [ 0.0213,  0.0105,  0.0099,  ..., -0.0788,  0.0286, -0.0173]]), '21.to_v_ip.weight': tensor([[-0.0428, -0.0086, -0.0299,  ..., -0.0137, -0.0076, -0.0231],\n",
      "        [-0.0046,  0.0686,  0.1279,  ...,  0.0335, -0.0069,  0.0226],\n",
      "        [-0.0447, -0.1342, -0.0020,  ..., -0.0584, -0.0346,  0.0157],\n",
      "        ...,\n",
      "        [-0.0233, -0.0009,  0.0431,  ...,  0.0250,  0.0081,  0.0048],\n",
      "        [ 0.0448, -0.0457, -0.0042,  ...,  0.0418, -0.0748,  0.0191],\n",
      "        [-0.0073, -0.1157, -0.0512,  ...,  0.0404, -0.0028, -0.0594]]), '19.to_k_ip.weight': tensor([[ 0.0200,  0.0665,  0.1233,  ...,  0.0066, -0.0320,  0.0745],\n",
      "        [-0.0555, -0.0729, -0.0080,  ...,  0.0391,  0.0406, -0.0810],\n",
      "        [-0.0339, -0.0409, -0.0292,  ..., -0.0444,  0.0446,  0.0711],\n",
      "        ...,\n",
      "        [-0.0690,  0.0140,  0.0388,  ..., -0.0199, -0.0448,  0.0648],\n",
      "        [ 0.0191, -0.0488, -0.0197,  ...,  0.0046,  0.0038, -0.0070],\n",
      "        [ 0.0235, -0.0995, -0.1022,  ...,  0.2043,  0.0282, -0.0124]]), '19.to_v_ip.weight': tensor([[-0.0319, -0.0216,  0.0671,  ..., -0.0457, -0.0596, -0.0041],\n",
      "        [ 0.0060,  0.0271,  0.0357,  ...,  0.0258,  0.0148, -0.0038],\n",
      "        [ 0.0350, -0.0255, -0.0447,  ...,  0.0123,  0.0350, -0.0244],\n",
      "        ...,\n",
      "        [ 0.0381,  0.0566,  0.0648,  ...,  0.0214,  0.0286,  0.0781],\n",
      "        [ 0.0221, -0.0507, -0.0492,  ...,  0.0091,  0.0053, -0.0630],\n",
      "        [ 0.0057, -0.0185,  0.0008,  ...,  0.0407,  0.0240,  0.0041]]), '15.to_k_ip.weight': tensor([[ 0.0203,  0.0107, -0.0789,  ...,  0.0307, -0.0342,  0.0327],\n",
      "        [-0.0335, -0.1006,  0.1376,  ..., -0.0242,  0.0407, -0.0355],\n",
      "        [-0.0722,  0.0131,  0.0338,  ..., -0.0331,  0.0061,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0450, -0.0041,  0.0025,  ..., -0.0040,  0.0317, -0.0727],\n",
      "        [-0.0326,  0.0909, -0.1125,  ..., -0.0258,  0.0600, -0.0102],\n",
      "        [ 0.0704, -0.0037,  0.0069,  ...,  0.0359,  0.0325, -0.0253]]), '15.to_v_ip.weight': tensor([[ 0.0063,  0.0227, -0.0371,  ...,  0.0194,  0.0199,  0.0013],\n",
      "        [ 0.0250,  0.0566, -0.0361,  ...,  0.0215,  0.0125, -0.0354],\n",
      "        [-0.0196, -0.0799, -0.0180,  ..., -0.0285, -0.0670, -0.0030],\n",
      "        ...,\n",
      "        [-0.0172,  0.0171,  0.0326,  ..., -0.0271, -0.0519, -0.0042],\n",
      "        [-0.0701, -0.0849,  0.0492,  ..., -0.0113, -0.0015, -0.0030],\n",
      "        [ 0.0426,  0.0600, -0.0016,  ...,  0.0159, -0.0441,  0.0682]]), '17.to_k_ip.weight': tensor([[-0.0446, -0.0120,  0.0424,  ..., -0.0297, -0.0904,  0.0221],\n",
      "        [-0.1105, -0.0178,  0.0074,  ..., -0.1004,  0.0246, -0.0247],\n",
      "        [ 0.0738,  0.0177, -0.0569,  ..., -0.0360,  0.0091, -0.0047],\n",
      "        ...,\n",
      "        [-0.0959, -0.1177,  0.0007,  ..., -0.0487,  0.0494,  0.0559],\n",
      "        [-0.0083, -0.0222, -0.0048,  ..., -0.0479, -0.0071,  0.0268],\n",
      "        [ 0.0937, -0.0169,  0.0151,  ..., -0.1010,  0.0126,  0.0042]]), '17.to_v_ip.weight': tensor([[-0.0293,  0.1225, -0.0496,  ..., -0.0473,  0.0025, -0.0150],\n",
      "        [-0.0196,  0.0002,  0.0392,  ...,  0.0069, -0.0251,  0.0008],\n",
      "        [-0.0823,  0.0167, -0.0568,  ...,  0.0081, -0.0085,  0.0123],\n",
      "        ...,\n",
      "        [-0.0057,  0.0845,  0.0821,  ..., -0.0252, -0.0276,  0.0195],\n",
      "        [ 0.0406,  0.0356,  0.0589,  ..., -0.0154,  0.0032, -0.0198],\n",
      "        [-0.0320, -0.0653,  0.0259,  ...,  0.0093, -0.0278,  0.0077]])}\n",
      "Saved new checkpoint to F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\transfer\\posectrl2.bin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "\n",
    "def change_checkpoint(checkpoint_path, new_checkpoint_path):\n",
    "    # 使用 safetensors 加载文件\n",
    "    sd = load_file(checkpoint_path)\n",
    "    \n",
    "    vpmatrix_points_sd = {}\n",
    "    atten_sd = {}\n",
    "    proj_sd = {}\n",
    "    \n",
    "    # 遍历模型权重并分类\n",
    "    i, j = 1 , 1\n",
    "    for k in sd:\n",
    "        # print(k)\n",
    "        if k.startswith(\"unet\"):\n",
    "            pass\n",
    "        elif k.startswith(\"vpmatrix_points\"):\n",
    "            vpmatrix_points_sd[k.replace(\"vpmatrix_points.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"atten_modules\"):\n",
    "            atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "        elif k.startswith(\"image_proj_model\"):\n",
    "            proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "\n",
    "    # 指定新的文件路径\n",
    "    new_checkpoint_path = Path(new_checkpoint_path, \"posectrl2.bin\")\n",
    "    \n",
    "    print(atten_sd)\n",
    "    \n",
    "    # 保存为新的二进制 checkpoint 文件\n",
    "    torch.save({\n",
    "        \"vpmatrix_points\": vpmatrix_points_sd,\n",
    "        \"atten_modules\": atten_sd,\n",
    "        \"image_proj_model\": proj_sd\n",
    "    }, new_checkpoint_path)\n",
    "    \n",
    "    print(f\"Saved new checkpoint to {new_checkpoint_path}\")\n",
    "\n",
    "# 使用 safetensors 文件路径\n",
    "ckpt = r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\model.safetensors\"\n",
    "\n",
    "# 调用转换函数\n",
    "change_checkpoint(ckpt, r\"F:\\Projects\\diffusers\\Project\\PoseCtrl\\sd-pose_ctrl\\transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d9129ecea4be1ae628b20f57dcc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "d:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'logit_scale', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13860, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_img2img.py:663: FutureWarning: You have passed 4 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.\n",
      "  deprecate(\"len(prompt) != len(image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9cbc3f82b3439081e9155b19c6b717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\diffusers\\image_processor.py:97: RuntimeWarning: invalid value encountered in cast\n",
      "  images = (images * 255).round().astype(\"uint8\")\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1024x256>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl')\n",
    "sys.path.append(r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl')\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "from poseCtrl.models.posectrl import PoseCtrl\n",
    "\n",
    "\n",
    "\n",
    "base_point_path=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt'\n",
    "raw_base_points=load_base_points(base_point_path)  \n",
    "\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt = r\"F:\\Projects\\diffusers\\Project\\sd-pose_ctrl\\posectrl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")\n",
    "\n",
    "path = r\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\"\n",
    "dataset = CustomDataset(path)\n",
    "data = dataset[0]\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Resize((256, 256))\n",
    "\n",
    "image = data['image']\n",
    "image = transform(image) \n",
    "g_image = data['feature']\n",
    "g_image = transform(g_image) \n",
    "\n",
    "vmatrix = data['view_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "pmatrix = data['projection_matrix'].to(torch.float16).unsqueeze(0).to(device)\n",
    "\n",
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42, image=g_image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13860, 4])\n",
      "{'31.to_k_ip.weight': tensor([[-0.0065,  0.0132, -0.0028,  ..., -0.0033,  0.0308,  0.0227],\n",
      "        [ 0.0080, -0.0480,  0.0373,  ..., -0.0060,  0.0455,  0.0155],\n",
      "        [-0.0357,  0.0118, -0.0137,  ...,  0.0052, -0.0451, -0.0260],\n",
      "        ...,\n",
      "        [-0.0271,  0.0085,  0.0243,  ..., -0.0136, -0.0057, -0.0120],\n",
      "        [ 0.0143,  0.0092, -0.0138,  ...,  0.0592,  0.0064, -0.0519],\n",
      "        [ 0.0661,  0.0127,  0.0268,  ..., -0.0758, -0.0880,  0.0991]]), '31.to_v_ip.weight': tensor([[ 0.0035,  0.0043,  0.0010,  ..., -0.0058, -0.0055,  0.0105],\n",
      "        [ 0.0067,  0.0251, -0.0053,  ...,  0.0087,  0.0110,  0.0010],\n",
      "        [-0.0129, -0.0031,  0.0133,  ...,  0.0007, -0.0104, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0086, -0.0129,  0.0013,  ...,  0.0094,  0.0074,  0.0191],\n",
      "        [-0.0027, -0.0009,  0.0189,  ..., -0.0010,  0.0096,  0.0106],\n",
      "        [ 0.0004,  0.0368,  0.0139,  ..., -0.0083, -0.0237,  0.0085]]), '29.to_k_ip.weight': tensor([[-0.0142, -0.0629, -0.0299,  ..., -0.0217, -0.0629,  0.0011],\n",
      "        [ 0.0497,  0.1232,  0.0318,  ...,  0.0021,  0.0501, -0.0198],\n",
      "        [-0.0192,  0.0201,  0.0523,  ...,  0.1008,  0.0459,  0.0308],\n",
      "        ...,\n",
      "        [-0.0990,  0.1024,  0.0437,  ..., -0.0269, -0.0546,  0.0282],\n",
      "        [ 0.0030, -0.0839, -0.0550,  ..., -0.0557,  0.0133,  0.0513],\n",
      "        [-0.0254,  0.0016,  0.0420,  ...,  0.0206,  0.0363, -0.0311]]), '29.to_v_ip.weight': tensor([[ 0.0013, -0.0088, -0.0085,  ...,  0.0070, -0.0071, -0.0181],\n",
      "        [ 0.0027, -0.0018, -0.0155,  ...,  0.0015, -0.0025, -0.0149],\n",
      "        [-0.0266,  0.0404, -0.0116,  ...,  0.0030,  0.0060, -0.0103],\n",
      "        ...,\n",
      "        [ 0.0111, -0.0251, -0.0026,  ...,  0.0015,  0.0053,  0.0239],\n",
      "        [-0.0100,  0.0062, -0.0011,  ...,  0.0005, -0.0253,  0.0104],\n",
      "        [-0.0022, -0.0543,  0.0113,  ..., -0.0140, -0.0075, -0.0110]]), '27.to_k_ip.weight': tensor([[-0.0031,  0.0199,  0.0647,  ..., -0.0173, -0.0584, -0.0356],\n",
      "        [-0.0315,  0.0535, -0.0163,  ..., -0.0302, -0.0748,  0.0178],\n",
      "        [ 0.0747, -0.1144, -0.0372,  ...,  0.0854, -0.0565,  0.0869],\n",
      "        ...,\n",
      "        [-0.0361,  0.0031,  0.0121,  ...,  0.0143, -0.0086,  0.0037],\n",
      "        [ 0.0309,  0.0124, -0.0894,  ..., -0.1055,  0.0091, -0.0852],\n",
      "        [ 0.1492, -0.0656, -0.0474,  ..., -0.0083, -0.0582,  0.0551]]), '27.to_v_ip.weight': tensor([[ 0.0163,  0.0160,  0.0021,  ..., -0.0145, -0.0019,  0.0280],\n",
      "        [-0.0102, -0.0209, -0.0065,  ..., -0.0236, -0.0129,  0.0051],\n",
      "        [-0.0157,  0.0280,  0.0036,  ..., -0.0103, -0.0099, -0.0186],\n",
      "        ...,\n",
      "        [ 0.0077, -0.0037, -0.0133,  ..., -0.0052,  0.0191,  0.0090],\n",
      "        [-0.0219,  0.0313,  0.0237,  ..., -0.0121, -0.0183,  0.0014],\n",
      "        [ 0.0031, -0.0056,  0.0184,  ..., -0.0057, -0.0174,  0.0091]]), '25.to_k_ip.weight': tensor([[ 0.0732,  0.0393, -0.0010,  ..., -0.0031, -0.0927,  0.0700],\n",
      "        [ 0.0193, -0.1816, -0.0213,  ...,  0.0214, -0.0043,  0.0260],\n",
      "        [-0.0665, -0.0488, -0.0016,  ...,  0.0619, -0.0117,  0.0256],\n",
      "        ...,\n",
      "        [-0.0493,  0.0203,  0.0108,  ..., -0.0701, -0.0003, -0.0514],\n",
      "        [-0.0092, -0.0170,  0.0254,  ...,  0.0133, -0.0094,  0.1019],\n",
      "        [ 0.0624,  0.0884, -0.0836,  ...,  0.0175,  0.0597, -0.0256]]), '25.to_v_ip.weight': tensor([[ 0.0093,  0.0340, -0.0021,  ...,  0.0338, -0.0004,  0.0497],\n",
      "        [-0.0051, -0.0713,  0.0467,  ..., -0.0184, -0.0141,  0.0166],\n",
      "        [-0.0160, -0.0321,  0.0733,  ..., -0.0211, -0.0107,  0.0591],\n",
      "        ...,\n",
      "        [-0.0473,  0.0328,  0.0782,  ..., -0.0023, -0.0009, -0.0697],\n",
      "        [-0.0069, -0.0474,  0.0818,  ..., -0.0042,  0.0396, -0.0092],\n",
      "        [-0.0117, -0.0240, -0.0042,  ...,  0.0059,  0.0125,  0.0783]]), '23.to_k_ip.weight': tensor([[-0.0382, -0.0471, -0.0337,  ..., -0.0189,  0.0250, -0.0227],\n",
      "        [ 0.0745,  0.0800, -0.0100,  ...,  0.0305,  0.0210, -0.0677],\n",
      "        [ 0.0437,  0.0276,  0.0473,  ...,  0.0425, -0.0102,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0157,  0.0568,  0.0312,  ...,  0.0144,  0.0026,  0.0479],\n",
      "        [ 0.0439,  0.0953,  0.0467,  ...,  0.0247, -0.0261, -0.0019],\n",
      "        [ 0.0440, -0.0157, -0.0961,  ...,  0.0819, -0.0407, -0.0648]]), '23.to_v_ip.weight': tensor([[-0.0067, -0.0321, -0.0221,  ...,  0.0344, -0.0115,  0.0260],\n",
      "        [ 0.0448, -0.0461,  0.0128,  ...,  0.0272, -0.0066,  0.0557],\n",
      "        [-0.0007,  0.0573,  0.0225,  ..., -0.0066, -0.0277, -0.0146],\n",
      "        ...,\n",
      "        [-0.0479,  0.0908, -0.1083,  ...,  0.0395, -0.0514, -0.0775],\n",
      "        [-0.0667,  0.0044,  0.1094,  ...,  0.0143, -0.0117, -0.0109],\n",
      "        [ 0.0627,  0.0769, -0.0594,  ..., -0.0052, -0.0301,  0.0342]]), '21.to_k_ip.weight': tensor([[-0.1159,  0.0307, -0.0336,  ...,  0.0027, -0.0221, -0.0651],\n",
      "        [ 0.0162,  0.0465,  0.0301,  ...,  0.0598, -0.0529, -0.0460],\n",
      "        [-0.0238, -0.0160,  0.0902,  ...,  0.0312,  0.1006,  0.0812],\n",
      "        ...,\n",
      "        [-0.0474,  0.0390,  0.0108,  ...,  0.0214,  0.0172, -0.0410],\n",
      "        [-0.0889, -0.0579,  0.0592,  ...,  0.0641,  0.0424,  0.0022],\n",
      "        [ 0.0213,  0.0105,  0.0099,  ..., -0.0788,  0.0286, -0.0173]]), '21.to_v_ip.weight': tensor([[-0.0428, -0.0086, -0.0299,  ..., -0.0137, -0.0076, -0.0231],\n",
      "        [-0.0046,  0.0686,  0.1279,  ...,  0.0335, -0.0069,  0.0226],\n",
      "        [-0.0447, -0.1342, -0.0020,  ..., -0.0584, -0.0346,  0.0157],\n",
      "        ...,\n",
      "        [-0.0233, -0.0009,  0.0431,  ...,  0.0250,  0.0081,  0.0048],\n",
      "        [ 0.0448, -0.0457, -0.0042,  ...,  0.0418, -0.0748,  0.0191],\n",
      "        [-0.0073, -0.1157, -0.0512,  ...,  0.0404, -0.0028, -0.0594]]), '19.to_k_ip.weight': tensor([[ 0.0200,  0.0665,  0.1233,  ...,  0.0066, -0.0320,  0.0745],\n",
      "        [-0.0555, -0.0729, -0.0080,  ...,  0.0391,  0.0406, -0.0810],\n",
      "        [-0.0339, -0.0409, -0.0292,  ..., -0.0444,  0.0446,  0.0711],\n",
      "        ...,\n",
      "        [-0.0690,  0.0140,  0.0388,  ..., -0.0199, -0.0448,  0.0648],\n",
      "        [ 0.0191, -0.0488, -0.0197,  ...,  0.0046,  0.0038, -0.0070],\n",
      "        [ 0.0235, -0.0995, -0.1022,  ...,  0.2043,  0.0282, -0.0124]]), '19.to_v_ip.weight': tensor([[-0.0319, -0.0216,  0.0671,  ..., -0.0457, -0.0596, -0.0041],\n",
      "        [ 0.0060,  0.0271,  0.0357,  ...,  0.0258,  0.0148, -0.0038],\n",
      "        [ 0.0350, -0.0255, -0.0447,  ...,  0.0123,  0.0350, -0.0244],\n",
      "        ...,\n",
      "        [ 0.0381,  0.0566,  0.0648,  ...,  0.0214,  0.0286,  0.0781],\n",
      "        [ 0.0221, -0.0507, -0.0492,  ...,  0.0091,  0.0053, -0.0630],\n",
      "        [ 0.0057, -0.0185,  0.0008,  ...,  0.0407,  0.0240,  0.0041]]), '15.to_k_ip.weight': tensor([[ 0.0203,  0.0107, -0.0789,  ...,  0.0307, -0.0342,  0.0327],\n",
      "        [-0.0335, -0.1006,  0.1376,  ..., -0.0242,  0.0407, -0.0355],\n",
      "        [-0.0722,  0.0131,  0.0338,  ..., -0.0331,  0.0061,  0.0284],\n",
      "        ...,\n",
      "        [ 0.0450, -0.0041,  0.0025,  ..., -0.0040,  0.0317, -0.0727],\n",
      "        [-0.0326,  0.0909, -0.1125,  ..., -0.0258,  0.0600, -0.0102],\n",
      "        [ 0.0704, -0.0037,  0.0069,  ...,  0.0359,  0.0325, -0.0253]]), '15.to_v_ip.weight': tensor([[ 0.0063,  0.0227, -0.0371,  ...,  0.0194,  0.0199,  0.0013],\n",
      "        [ 0.0250,  0.0566, -0.0361,  ...,  0.0215,  0.0125, -0.0354],\n",
      "        [-0.0196, -0.0799, -0.0180,  ..., -0.0285, -0.0670, -0.0030],\n",
      "        ...,\n",
      "        [-0.0172,  0.0171,  0.0326,  ..., -0.0271, -0.0519, -0.0042],\n",
      "        [-0.0701, -0.0849,  0.0492,  ..., -0.0113, -0.0015, -0.0030],\n",
      "        [ 0.0426,  0.0600, -0.0016,  ...,  0.0159, -0.0441,  0.0682]]), '17.to_k_ip.weight': tensor([[-0.0446, -0.0120,  0.0424,  ..., -0.0297, -0.0904,  0.0221],\n",
      "        [-0.1105, -0.0178,  0.0074,  ..., -0.1004,  0.0246, -0.0247],\n",
      "        [ 0.0738,  0.0177, -0.0569,  ..., -0.0360,  0.0091, -0.0047],\n",
      "        ...,\n",
      "        [-0.0959, -0.1177,  0.0007,  ..., -0.0487,  0.0494,  0.0559],\n",
      "        [-0.0083, -0.0222, -0.0048,  ..., -0.0479, -0.0071,  0.0268],\n",
      "        [ 0.0937, -0.0169,  0.0151,  ..., -0.1010,  0.0126,  0.0042]]), '17.to_v_ip.weight': tensor([[-0.0293,  0.1225, -0.0496,  ..., -0.0473,  0.0025, -0.0150],\n",
      "        [-0.0196,  0.0002,  0.0392,  ...,  0.0069, -0.0251,  0.0008],\n",
      "        [-0.0823,  0.0167, -0.0568,  ...,  0.0081, -0.0085,  0.0123],\n",
      "        ...,\n",
      "        [-0.0057,  0.0845,  0.0821,  ..., -0.0252, -0.0276,  0.0195],\n",
      "        [ 0.0406,  0.0356,  0.0589,  ..., -0.0154,  0.0032, -0.0198],\n",
      "        [-0.0320, -0.0653,  0.0259,  ...,  0.0093, -0.0278,  0.0077]])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ModuleList:\n\tMissing key(s) in state_dict: \"1.to_k_ip.weight\", \"1.to_v_ip.weight\", \"3.to_k_ip.weight\", \"3.to_v_ip.weight\", \"5.to_k_ip.weight\", \"5.to_v_ip.weight\", \"7.to_k_ip.weight\", \"7.to_v_ip.weight\", \"9.to_k_ip.weight\", \"9.to_v_ip.weight\", \"11.to_k_ip.weight\", \"11.to_v_ip.weight\", \"13.to_k_ip.weight\", \"13.to_v_ip.weight\". \n\tsize mismatch for 19.to_k_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 19.to_v_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 25.to_k_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 25.to_v_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 31.to_k_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).\n\tsize mismatch for 31.to_v_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pose_model \u001b[38;5;241m=\u001b[39m \u001b[43mPoseCtrl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_encoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip_ckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_base_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m images \u001b[38;5;241m=\u001b[39m pose_model\u001b[38;5;241m.\u001b[39mgenerate(pil_image\u001b[38;5;241m=\u001b[39mimage, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, image\u001b[38;5;241m=\u001b[39mg_image, strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, V_matrix\u001b[38;5;241m=\u001b[39mvmatrix,P_matrix\u001b[38;5;241m=\u001b[39mpmatrix )\n\u001b[0;32m      3\u001b[0m grid \u001b[38;5;241m=\u001b[39m image_grid(images, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mf:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl\\models\\posectrl.py:40\u001b[0m, in \u001b[0;36mPoseCtrl.__init__\u001b[1;34m(self, sd_pipe, image_encoder_path, pose_ckpt, raw_base_points, device, num_tokens)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvpmatrix_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_VP()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_proj_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_proj()\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_posectrl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Projects\\diffusers\\Project\\PoseCtrl\\poseCtrl\\models\\posectrl.py:102\u001b[0m, in \u001b[0;36mPoseCtrl.load_posectrl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvpmatrix_points\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpmatrix_points\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    101\u001b[0m atten_layers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mattn_processors\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 102\u001b[0m \u001b[43matten_layers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43matten_modules\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ModuleList:\n\tMissing key(s) in state_dict: \"1.to_k_ip.weight\", \"1.to_v_ip.weight\", \"3.to_k_ip.weight\", \"3.to_v_ip.weight\", \"5.to_k_ip.weight\", \"5.to_v_ip.weight\", \"7.to_k_ip.weight\", \"7.to_v_ip.weight\", \"9.to_k_ip.weight\", \"9.to_v_ip.weight\", \"11.to_k_ip.weight\", \"11.to_v_ip.weight\", \"13.to_k_ip.weight\", \"13.to_v_ip.weight\". \n\tsize mismatch for 19.to_k_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 19.to_v_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 25.to_k_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 25.to_v_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 31.to_k_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).\n\tsize mismatch for 31.to_v_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768])."
     ]
    }
   ],
   "source": [
    "pose_model = PoseCtrl(pipe, image_encoder_path, ip_ckpt, raw_base_points, device)\n",
    "images = pose_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42, image=g_image, strength=0.6, V_matrix=vmatrix,P_matrix=pmatrix )\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "import pickle\n",
    "\n",
    "def convert_sd_weights_to_bin(folder_path, output_bin_path):\n",
    "    \"\"\"\n",
    "    将 Stable Diffusion 文件夹中的多个权重文件（safetensors, bin, pkl, pt）合并并保存为 .bin 格式。\n",
    "\n",
    "    :param folder_path: 包含权重文件的文件夹路径\n",
    "    :param output_bin_path: 输出的 .bin 文件路径\n",
    "    \"\"\"\n",
    "    merged_state_dict = {}  # 存储所有权重的字典\n",
    "\n",
    "    # 遍历文件夹，找到所有权重文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.safetensors'):\n",
    "            print(f\"Loading {file_path} (safetensors)...\")\n",
    "            state_dict = load_file(file_path, device=\"cpu\")\n",
    "        elif filename.endswith('.bin'):\n",
    "            print(f\"Loading {file_path} (bin)...\")\n",
    "            state_dict = torch.load(file_path, map_location=\"cpu\")\n",
    "        elif filename.endswith('.pkl'):\n",
    "            print(f\"Loading {file_path} (pkl)...\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                state_dict = pickle.load(f)\n",
    "        elif filename.endswith('.pt'):\n",
    "            print(f\"Loading {file_path} (pt)...\")\n",
    "            state_dict = torch.load(file_path, map_location=\"cpu\")\n",
    "        else:\n",
    "            print(f\"Skipping {file_path}, unsupported format.\")\n",
    "            continue\n",
    "\n",
    "        # 合并权重（如果存在相同的 key，则不会覆盖）\n",
    "        for key, value in state_dict.items():\n",
    "            if key not in merged_state_dict:\n",
    "                merged_state_dict[key] = value\n",
    "\n",
    "    # 保存到 .bin 文件\n",
    "    torch.save(merged_state_dict, output_bin_path)\n",
    "    print(f\"Saved merged weights to {output_bin_path}\")\n",
    "\n",
    "# 示例用法\n",
    "folder_path = r\"/content/sd-pose_ctrl/checkpoint-20\"  # 替换为你的文件夹路径\n",
    "output_bin_path = r\"/content/sd-pose_ctrl/checkpoint-20/stable_diffusion.bin\"  # 目标 bin 文件路径\n",
    "\n",
    "convert_sd_weights_to_bin(folder_path, output_bin_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed shared tensor {'atten_modules.27.to_v_ip.weight', 'atten_modules.19.to_v_ip.weight', 'atten_modules.1.to_k_ip.weight', 'atten_modules.29.to_k_ip.weight', 'atten_modules.5.to_k_ip.weight', 'atten_modules.23.to_k_ip.weight', 'atten_modules.11.to_v_ip.weight', 'atten_modules.3.to_k_ip.weight', 'atten_modules.9.to_v_ip.weight', 'atten_modules.9.to_k_ip.weight', 'atten_modules.13.to_v_ip.weight', 'atten_modules.27.to_k_ip.weight', 'atten_modules.3.to_v_ip.weight', 'atten_modules.15.to_v_ip.weight', 'atten_modules.25.to_k_ip.weight', 'atten_modules.31.to_k_ip.weight', 'atten_modules.25.to_v_ip.weight', 'atten_modules.7.to_k_ip.weight', 'atten_modules.15.to_k_ip.weight', 'atten_modules.21.to_v_ip.weight', 'atten_modules.19.to_k_ip.weight', 'atten_modules.11.to_k_ip.weight', 'atten_modules.7.to_v_ip.weight', 'atten_modules.13.to_k_ip.weight', 'atten_modules.17.to_k_ip.weight', 'atten_modules.1.to_v_ip.weight', 'atten_modules.29.to_v_ip.weight', 'atten_modules.17.to_v_ip.weight', 'atten_modules.5.to_v_ip.weight', 'atten_modules.21.to_k_ip.weight', 'atten_modules.31.to_v_ip.weight', 'atten_modules.23.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python train_colab.py --save_steps 2\n",
    "2025-02-17 10:33:25.135034: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "2025-02-17 10:33:25.153738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
    "E0000 00:00:1739788405.175769    6816 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "E0000 00:00:1739788405.182549    6816 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "2025-02-17 10:33:25.204841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "Traceback (most recent call last):\n",
    "  File \"/content/train_colab.py\", line 346, in <module>\n",
    "    main()  \n",
    "    ^^^^^^\n",
    "  File \"/content/train_colab.py\", line 286, in main\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, atten_modules, optimizer, train_dataloader)\n",
    "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Removed shared tensor {'atten_modules.27.to_v_ip.weight', 'atten_modules.19.to_v_ip.weight', 'atten_modules.1.to_k_ip.weight', 'atten_modules.29.to_k_ip.weight', 'atten_modules.5.to_k_ip.weight', 'atten_modules.23.to_k_ip.weight', 'atten_modules.11.to_v_ip.weight', 'atten_modules.3.to_k_ip.weight', 'atten_modules.9.to_v_ip.weight', 'atten_modules.9.to_k_ip.weight', 'atten_modules.13.to_v_ip.weight', 'atten_modules.27.to_k_ip.weight', 'atten_modules.3.to_v_ip.weight', 'atten_modules.15.to_v_ip.weight', 'atten_modules.25.to_k_ip.weight', 'atten_modules.31.to_k_ip.weight', 'atten_modules.25.to_v_ip.weight', 'atten_modules.7.to_k_ip.weight', 'atten_modules.15.to_k_ip.weight', 'atten_modules.21.to_v_ip.weight', 'atten_modules.19.to_k_ip.weight', 'atten_modules.11.to_k_ip.weight', 'atten_modules.7.to_v_ip.weight', 'atten_modules.13.to_k_ip.weight', 'atten_modules.17.to_k_ip.weight', 'atten_modules.1.to_v_ip.weight', 'atten_modules.29.to_v_ip.weight', 'atten_modules.17.to_v_ip.weight', 'atten_modules.5.to_v_ip.weight', 'atten_modules.21.to_k_ip.weight', 'atten_modules.31.to_v_ip.weight', 'atten_modules.23.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
