{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet 2d condition model\n",
    "\n",
    "[unet](https://huggingface.co/docs/diffusers/en/api/models/unet2d-cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"  # Stable Diffusion v1.5\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "unet_sd = unet.state_dict()2\n",
    "# for key, value in unet_sd.items():\n",
    "#     print(key, value.shape)\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    if cross_attention_dim is None:\n",
    "        pass\n",
    "    else:\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.0.1+cu118\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: filelock, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, torchaudio, torchvision\n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.15.2+cu118\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: d:\\my_sotfware\\anaconda\\envs\\cameractrl\\lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('sample_size', 64),\n",
       "            ('in_channels', 4),\n",
       "            ('out_channels', 4),\n",
       "            ('center_input_sample', False),\n",
       "            ('flip_sin_to_cos', True),\n",
       "            ('freq_shift', 0),\n",
       "            ('down_block_types',\n",
       "             ['CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'CrossAttnDownBlock2D',\n",
       "              'DownBlock2D']),\n",
       "            ('mid_block_type', 'UNetMidBlock2DCrossAttn'),\n",
       "            ('up_block_types',\n",
       "             ['UpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D',\n",
       "              'CrossAttnUpBlock2D']),\n",
       "            ('only_cross_attention', False),\n",
       "            ('block_out_channels', [320, 640, 1280, 1280]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('downsample_padding', 1),\n",
       "            ('mid_block_scale_factor', 1),\n",
       "            ('dropout', 0.0),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('norm_eps', 1e-05),\n",
       "            ('cross_attention_dim', 768),\n",
       "            ('transformer_layers_per_block', 1),\n",
       "            ('reverse_transformer_layers_per_block', None),\n",
       "            ('encoder_hid_dim', None),\n",
       "            ('encoder_hid_dim_type', None),\n",
       "            ('attention_head_dim', 8),\n",
       "            ('num_attention_heads', None),\n",
       "            ('dual_cross_attention', False),\n",
       "            ('use_linear_projection', False),\n",
       "            ('class_embed_type', None),\n",
       "            ('addition_embed_type', None),\n",
       "            ('addition_time_embed_dim', None),\n",
       "            ('num_class_embeds', None),\n",
       "            ('upcast_attention', False),\n",
       "            ('resnet_time_scale_shift', 'default'),\n",
       "            ('resnet_skip_time_act', False),\n",
       "            ('resnet_out_scale_factor', 1.0),\n",
       "            ('time_embedding_type', 'positional'),\n",
       "            ('time_embedding_dim', None),\n",
       "            ('time_embedding_act_fn', None),\n",
       "            ('timestep_post_act', None),\n",
       "            ('time_cond_proj_dim', None),\n",
       "            ('conv_in_kernel', 3),\n",
       "            ('conv_out_kernel', 3),\n",
       "            ('projection_class_embeddings_input_dim', None),\n",
       "            ('attention_type', 'default'),\n",
       "            ('class_embeddings_concat', False),\n",
       "            ('mid_block_only_cross_attention', None),\n",
       "            ('cross_attention_norm', None),\n",
       "            ('addition_embed_type_num_heads', 64),\n",
       "            ('_use_default_values',\n",
       "             ['num_attention_heads',\n",
       "              'encoder_hid_dim',\n",
       "              'mid_block_only_cross_attention',\n",
       "              'mid_block_type',\n",
       "              'time_embedding_dim',\n",
       "              'timestep_post_act',\n",
       "              'resnet_skip_time_act',\n",
       "              'time_embedding_act_fn',\n",
       "              'attention_type',\n",
       "              'projection_class_embeddings_input_dim',\n",
       "              'num_class_embeds',\n",
       "              'upcast_attention',\n",
       "              'addition_embed_type',\n",
       "              'cross_attention_norm',\n",
       "              'class_embed_type',\n",
       "              'dual_cross_attention',\n",
       "              'transformer_layers_per_block',\n",
       "              'resnet_time_scale_shift',\n",
       "              'dropout',\n",
       "              'conv_in_kernel',\n",
       "              'conv_out_kernel',\n",
       "              'reverse_transformer_layers_per_block',\n",
       "              'encoder_hid_dim_type',\n",
       "              'resnet_out_scale_factor',\n",
       "              'addition_time_embed_dim',\n",
       "              'only_cross_attention',\n",
       "              'time_embedding_type',\n",
       "              'time_cond_proj_dim',\n",
       "              'addition_embed_type_num_heads',\n",
       "              'class_embeddings_concat',\n",
       "              'use_linear_projection']),\n",
       "            ('_class_name', 'UNet2DConditionModel'),\n",
       "            ('_diffusers_version', '0.6.0'),\n",
       "            ('_name_or_path', 'runwayml/stable-diffusion-v1-5')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577640>,\n",
       " 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2577cd0>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5420>,\n",
       " 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e5ab0>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7700>,\n",
       " 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a25e7d90>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26254e0>,\n",
       " 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2625b70>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627790>,\n",
       " 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2627e20>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659570>,\n",
       " 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2659c00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c8a00>,\n",
       " 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26c9090>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26caa10>,\n",
       " 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26cb0a0>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fca00>,\n",
       " 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fd090>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26fecb0>,\n",
       " 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a26ff340>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2730ca0>,\n",
       " 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2731330>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2732c50>,\n",
       " 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27332e0>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2764f40>,\n",
       " 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27653c0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2765fc0>,\n",
       " 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766080>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a27667a0>,\n",
       " 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2766860>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690490>,\n",
       " 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x209a2690b20>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet.attn_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixEncoder, VPmatrixPoints\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import MyDataset, load_base_points\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_pose_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to pretrained  posectrl model. If not specified weights are initialized randomly.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--base_point_path\",\n",
    "        type=str,\n",
    "        default=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt',\n",
    "        help='Path to base model points'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\pic\",\n",
    "        required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-pose_ctrl\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "class posectrl(nn.Module):\n",
    "    def __init__(self, unet, vpmatrix_points, atten_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.vpmatrix_points = vpmatrix_points\n",
    "        self.atten_modules = atten_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, V_matrix, P_matrix):\n",
    "        point_tokens = self.vpmatrix_points(V_matrix, P_matrix)\n",
    "        \"\"\" 修改:防止之后要加text \"\"\"\n",
    "        if encoder_hidden_states:\n",
    "            encoder_hidden_states = torch.cat([encoder_hidden_states, point_tokens], dim=1)\n",
    "        else:\n",
    "            encoder_hidden_states=point_tokens\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        orig_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.vpmatrix_points.load_state_dict(state_dict[\"vpmatrix_points\"], strict=True)\n",
    "        self.atten_modules.load_state_dict(state_dict[\"atten_modules\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        new_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_VPmatrix_sum != new_VPmatrix_sum, \"Weights of VPmatrixEncoder did not change!\"\n",
    "        assert orig_atten_sum != new_atten_sum, \"Weights of atten_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    raw_base_points=load_base_points(args.base_point_path)  \n",
    "    vpmatrix_points_sd = VPmatrixEncoder(raw_base_points)\n",
    "\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    atten_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    pose_ctrl = posectrl(unet, vpmatrix_points_sd, atten_modules, args.pretrained_pose_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(pose_ctrl.vpmatrix_points_sd.parameters(),  pose_ctrl.atten_modules.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = MyDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(pose_ctrl):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"image\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                if \"text_input_ids\" in batch:\n",
    "                    with torch.no_grad():\n",
    "                        encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                else:\n",
    "                    encoder_hidden_states=None\n",
    "                \n",
    "                noise_pred = pose_ctrl(noisy_latents, timesteps, encoder_hidden_states, batch['view_matrix'], batch['projection_matrix'])\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "里面结构没改\n",
    "\n",
    "应该有两个输入：图像和VP\n",
    "\n",
    "VP： VPmatrixEncoder -> [77,77]\n",
    "\n",
    "image: \n",
    "- :vae ->latent\n",
    "- :resampler? 写一个 vit或者别的网络\n",
    "- 那么训练参数会变成三个\n",
    "\n",
    "\n",
    "# TODO\n",
    "- 1. visEncoder.py\n",
    "- 2. attention_processor.py： 加逻辑\n",
    "- 3. posectrl.py: 加参数和逻辑\n",
    "- 4. main.py：加参数\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"checkpoint-50000/pytorch_model.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "VPmatrixEncoder_sd = {}\n",
    "atten_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"VPmatrixEncoder\"):\n",
    "        VPmatrixEncoder_sd[k.replace(\"VPmatrixEncoder.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"atten_modules\"):\n",
    "        atten_sd[k.replace(\"atten_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"VPmatrixEncoder\": VPmatrixEncoder_sd, \"atten_modules\": atten_sd}, \"posectrl.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 位置矩阵\n",
    "2. 本地的坐标：可以和vp矩阵相乘，数学意义，M矩阵，\n",
    "3. 多样性一点：图的特征，加上正面原图随便的特征。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. 反向：训练什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO NEW Version 1\n",
    "\n",
    "- inference\n",
    "- VP矩阵不需要处理了\n",
    "- BasePoints: [2000,4,4] @ [4,4] -> <77 768>（这个是text attention之后的结果，不知道图片他们都是怎么做的）可能可以换个大小，可学习的部分直接写出来更换就行。\n",
    "- 好像流程就没啥问题了\n",
    "- 每个要改的地方加上\"修改\",不然找不到忘记了.\n",
    "\n",
    "# TODO NEW Version 2\n",
    "   现在的逻辑是： vp矩阵[4,4], 顶点是[13860,4] (77x180), ->[13840,4] reshape [77,768]\n",
    "-  要改：\n",
    "   \n",
    "~~- base_load~~\n",
    "\n",
    "~~- pose_adaptor~~\n",
    "\n",
    "   ~~- posectrl traning~~ \n",
    "   - 和 posectrl inference   \n",
    " \n",
    "   ~~- attention_pocessor~~ 和之前没区别\n",
    "\n",
    "   ~~- train main~~\n",
    "\n",
    "   ~~- 跑通~~\n",
    "\n",
    "# TODO NEW Version 3\n",
    "加了个参考图， 这个图attention 加上\n",
    "\n",
    "~~- dataset 1024 resize~~, 可以不加数量限制\n",
    "- image sampler\n",
    "- posectrl train main\n",
    "- attention\n",
    "- inference\n",
    "\n",
    "# Questions\n",
    "~~1. 需不要把好坏prompt设置成~~\n",
    " \n",
    "python \n",
    "```\n",
    "if prompt is None:\n",
    "    prompt = \"best quality, high quality\"\n",
    "if negative_prompt is None:\n",
    "    negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "```\n",
    "\n",
    "不用应该\n",
    "\n",
    "- 2. point-e好像是生成3d底模的东西,不知道有没有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch feature shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 选择 CLIP 预训练模型\n",
    "model_name = \"openai/clip-vit-base-patch16\"  # 也可以换成 \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. 读取并预处理图片\n",
    "image_path = r\"F:\\Projects\\diffusers\\ProgramData\\sample_new\\NPC_Avatar_Girl_Sword_Ayaka\\feature.png\"  # 替换成你的图片路径\n",
    "image = Image.open(image_path)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")  # 预处理\n",
    "image_tensor = inputs[\"pixel_values\"]  # 获取输入张量，形状 (1, 3, 224, 224)\n",
    "\n",
    "# 3. 获取所有 patch 的特征\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.vision_model(image_tensor)  # 获取所有 Transformer 层的输出\n",
    "    patch_features = vision_outputs.last_hidden_state  # 形状: (B, X+1, 768)\n",
    "\n",
    "# 4. 移除 CLS token（第一个 token）\n",
    "patch_features = patch_features[:, 1:, :]  # (B, X, 768)\n",
    "\n",
    "print(\"Patch feature shape:\", patch_features.shape)  # 目标形状: (B, X, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "# FFN\n",
    "def FeedForward(dim, mult=4):\n",
    "    inner_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, inner_dim, bias=False),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(inner_dim, dim, bias=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def reshape_tensor(x, heads):\n",
    "    bs, length, width = x.shape\n",
    "    # (bs, length, width) --> (bs, length, n_heads, dim_per_head)\n",
    "    x = x.view(bs, length, heads, -1)\n",
    "    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n",
    "    x = x.transpose(1, 2)\n",
    "    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n",
    "    x = x.reshape(bs, heads, length, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(self, *, dim, dim_head=64, heads=8):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.dim_head = dim_head\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, latents):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): image features\n",
    "                shape (b, n1, D)\n",
    "            latent (torch.Tensor): latent features\n",
    "                shape (b, n2, D)\n",
    "        \"\"\"\n",
    "        x = self.norm1(x)\n",
    "        latents = self.norm2(latents)\n",
    "\n",
    "        b, l, _ = latents.shape\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "        kv_input = torch.cat((x, latents), dim=-2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
    "\n",
    "        q = reshape_tensor(q, self.heads)\n",
    "        k = reshape_tensor(k, self.heads)\n",
    "        v = reshape_tensor(v, self.heads)\n",
    "\n",
    "        # attention\n",
    "        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n",
    "        weight = (q * scale) @ (k * scale).transpose(-2, -1)  # More stable with f16 than dividing afterwards\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        out = weight @ v\n",
    "\n",
    "        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Resampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=1024,\n",
    "        depth=8,\n",
    "        dim_head=64,\n",
    "        heads=16,\n",
    "        num_queries=8,\n",
    "        embedding_dim=768,\n",
    "        output_dim=1024,\n",
    "        ff_mult=4,\n",
    "        max_seq_len: int = 257,  # CLIP tokens + CLS token\n",
    "        apply_pos_emb: bool = False,\n",
    "        num_latents_mean_pooled: int = 0,  # number of latents derived from mean pooled representation of the sequence\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, embedding_dim) if apply_pos_emb else None\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim**0.5)\n",
    "\n",
    "        self.proj_in = nn.Linear(embedding_dim, dim)\n",
    "\n",
    "        self.proj_out = nn.Linear(dim, output_dim)\n",
    "        self.norm_out = nn.LayerNorm(output_dim)\n",
    "\n",
    "        self.to_latents_from_mean_pooled_seq = (\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
    "                Rearrange(\"b (n d) -> b n d\", n=num_latents_mean_pooled),\n",
    "            )\n",
    "            if num_latents_mean_pooled > 0\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
    "                        FeedForward(dim=dim, mult=ff_mult),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pos_emb is not None:\n",
    "            n, device = x.shape[1], x.device\n",
    "            pos_emb = self.pos_emb(torch.arange(n, device=device))\n",
    "            x = x + pos_emb\n",
    "\n",
    "        latents = self.latents.repeat(x.size(0), 1, 1)\n",
    "\n",
    "        x = self.proj_in(x)\n",
    "\n",
    "        if self.to_latents_from_mean_pooled_seq:\n",
    "            meanpooled_seq = masked_mean(x, dim=1, mask=torch.ones(x.shape[:2], device=x.device, dtype=torch.bool))\n",
    "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
    "            latents = torch.cat((meanpooled_latents, latents), dim=-2)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x, latents) + latents\n",
    "            latents = ff(latents) + latents\n",
    "\n",
    "        latents = self.proj_out(latents)\n",
    "        return self.norm_out(latents)\n",
    "\n",
    "\n",
    "def masked_mean(t, *, dim, mask=None):\n",
    "    if mask is None:\n",
    "        return t.mean(dim=dim)\n",
    "\n",
    "    denom = mask.sum(dim=dim, keepdim=True)\n",
    "    mask = rearrange(mask, \"b n -> b n 1\")\n",
    "    masked_t = t.masked_fill(~mask, 0.0)\n",
    "\n",
    "    return masked_t.sum(dim=dim) / denom.clamp(min=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip_tokens shape: torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE = 2\n",
    "# # OUTPUT_DIM = 1280\n",
    "# OUTPUT_DIM = 768\n",
    "# NUM_QUERIES = 8\n",
    "# NUM_LATENTS_MEAN_POOLED = 0  # 0 for no mean pooling (previous behavior)\n",
    "# APPLY_POS_EMB = True  # False for no positional embeddings (previous behavior)\n",
    "# IMAGE_ENCODER_NAME_OR_PATH = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "image_proj_model = Resampler(\n",
    "        dim=1024,\n",
    "        depth=2,\n",
    "        dim_head=64,\n",
    "        heads=16,\n",
    "        num_queries=8,\n",
    "        embedding_dim=768,\n",
    "        output_dim=768,\n",
    "        ff_mult=2,\n",
    "        max_seq_len=257,\n",
    "        apply_pos_emb=False,\n",
    "        num_latents_mean_pooled=0,\n",
    "    )\n",
    "\n",
    "with torch.no_grad():\n",
    "    ip_tokens = image_proj_model(patch_features)\n",
    "print(\"ip_tokens shape:\", ip_tokens.shape)\n",
    "assert ip_tokens.shape == (1, 8, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n",
      "torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6874c03dff413bba56ec6bdebefa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MY_SOTFWARE\\anaconda\\envs\\cameractrl\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\30631\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-H-14-laion2B-s32B-b79K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d2935159a844fab353d31d0f101f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6089451fb52f4eb9800c0dd4b968fdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d4245d944d4d7bba393ae786233d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7767cb1ef1c49e1a5d205e780be6cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8f15c6e7fc4f1484ad5e56ff76f17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'logit_scale', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images and txt files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((512, 512)),  \n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "        self.samples = []\n",
    "\n",
    "        for folder_name in os.listdir(root_dir):\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                data_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "                image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg')) and f.lower().startswith('capture')]\n",
    "                feature = os.path.join(folder_path, \"feature.png\")\n",
    "                if not os.path.exists(feature):\n",
    "                    raise FileNotFoundError(f\"'{feature}' does not exist, please check again.\")\n",
    "                if len(data_files) == 134 and len(image_files) == 132:\n",
    "                    projection_matrix_file = None\n",
    "                    view_matrix_file = None\n",
    "                    for data_file in data_files:\n",
    "                        if 'projectionMatrix' in data_file:\n",
    "                            projection_matrix_file = os.path.join(folder_path, data_file)\n",
    "                        elif 'viewMatrix' in data_file:\n",
    "                            view_matrix_file = os.path.join(folder_path, data_file)\n",
    "                    image_files = [os.path.join(folder_path, img) for img in image_files]\n",
    "                    if projection_matrix_file and view_matrix_file and image_files:\n",
    "                        # 修改为每个图片与对应的矩阵文件配对\n",
    "                        projection_matrices = self.read_matrices(projection_matrix_file)\n",
    "                        view_matrices = self.read_matrices(view_matrix_file)\n",
    "                        self.samples.extend([(proj, view, img, feature) for proj, view, img in zip(projection_matrices, view_matrices, image_files)])\n",
    "                        # 添加调试信息\n",
    "                        # print(f\"Folder: {folder_name}, Number of images: {len(image_files)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        projection_matrix, view_matrix, image_file, feature_file = self.samples[idx]\n",
    "\n",
    "        # 确保图片文件路径正确\n",
    "        if not os.path.exists(image_file):\n",
    "            raise FileNotFoundError(f\"Image file not found: {image_file}\")\n",
    "\n",
    "        # 读取图像\n",
    "        try:\n",
    "            image = Image.open(image_file).convert('RGB')\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Error opening image file {image_file}: {e}\")\n",
    "        \n",
    "        if not os.path.exists(feature_file):\n",
    "            raise FileNotFoundError(f\"Image file not found: {feature_file}\")\n",
    "\n",
    "        # 读取图像\n",
    "        try:\n",
    "            feature = Image.open(feature_file).convert('RGB')\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Error opening image file {feature_file}: {e}\")\n",
    "\n",
    "\n",
    "        # 处理图像\n",
    "        image = self.transform(image)  # **确保转换成 Tensor**\n",
    "        feature = self.transform(feature)\n",
    "        # 确保矩阵是 Tensor\n",
    "        projection_matrix = torch.tensor(projection_matrix, dtype=torch.float32)\n",
    "        view_matrix = torch.tensor(view_matrix, dtype=torch.float32)\n",
    "\n",
    "        # 确保 projection_matrix 和 view_matrix 形状正确\n",
    "        if projection_matrix.shape != (4, 4):\n",
    "            raise ValueError(f\"Projection matrix shape is incorrect: {projection_matrix.shape}\")\n",
    "        if view_matrix.shape != (4, 4):\n",
    "            raise ValueError(f\"View matrix shape is incorrect: {view_matrix.shape}\")\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'projection_matrix': projection_matrix,\n",
    "            'view_matrix': view_matrix,\n",
    "            'feature': feature\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def read_matrices(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            matrices = []\n",
    "            matrix = []\n",
    "            for line in lines:\n",
    "                if 'Capture' not in line:  # 跳过包含 'Capture' 的行\n",
    "                    try:\n",
    "                        row = list(map(float, line.strip().split()))\n",
    "                        if len(row) == 4:  # 确保每一行有4个元素\n",
    "                            matrix.append(row)\n",
    "                            if len(matrix) == 4:  # 确保矩阵有4行\n",
    "                                matrices.append(np.array(matrix))\n",
    "                                matrix = []\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            return matrices\n",
    "        \n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "class ImageProjModel(torch.nn.Module):\n",
    "    \"\"\"Projection Model\"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
    "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
    "        )\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens\n",
    "\n",
    "train_dataset = CustomDataset(\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0]['feature'].shape)\n",
    "processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "inputs = processor(images=train_dataset[0]['feature'], return_tensors=\"pt\") \n",
    "image_tensor = inputs[\"pixel_values\"] \n",
    "image_embeds = image_encoder(image_tensor).image_embeds\n",
    "image_proj_model = ImageProjModel(\n",
    "        cross_attention_dim=768,\n",
    "        clip_embeddings_dim=1024,\n",
    "        clip_extra_context_tokens=4,\n",
    "    )\n",
    "ip_tokens = image_proj_model(image_embeds)\n",
    "print(image_embeds.shape)\n",
    "print(ip_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPProcessor\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "from poseCtrl.models.pose_adaptor import VPmatrixPoints, ImageProjModel\n",
    "from poseCtrl.models.attention_processor import AttnProcessor, PoseAttnProcessor\n",
    "from poseCtrl.data.dataset import CustomDataset, load_base_points\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str, \n",
    "        default='runwayml/stable-diffusion-v1-5',\n",
    "        # required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_pose_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to pretrained  posectrl model. If not specified weights are initialized randomly.\",\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     \"--data_json_file\",\n",
    "    #     type=str,\n",
    "    #     default=None,\n",
    "    #     required=True,\n",
    "    #     help=\"Training data\",\n",
    "    # )\n",
    "    parser.add_argument(\n",
    "        \"--base_point_path\",\n",
    "        type=str,\n",
    "        default=r'F:\\Projects\\diffusers\\Project\\PoseCtrl\\dataSet\\standardVertex.txt',\n",
    "        help='Path to base model points'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data_root_path\",\n",
    "        type=str,\n",
    "        default=\"F:\\\\Projects\\\\diffusers\\\\ProgramData\\\\sample_new\",\n",
    "        # required=True,\n",
    "        help=\"Training data root path\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_encoder_path\",\n",
    "        type=str,\n",
    "        default=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "        # required=True,\n",
    "        help=\"Path to CLIP image encoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"sd-pose_ctrl\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=1e-4,\n",
    "        help=\"Learning rate to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\",\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    return args\n",
    "\n",
    "class posectrl(nn.Module):\n",
    "    def __init__(self, unet, vpmatrix_points, image_proj_model, atten_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.vpmatrix_points = vpmatrix_points\n",
    "        self.atten_modules = atten_modules\n",
    "        self.image_proj_model = image_proj_model\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, V_matrix, P_matrix, image_embeds):\n",
    "        point_tokens = self.vpmatrix_points(V_matrix, P_matrix)\n",
    "        feature_tokens = self.image_proj_model(image_embeds)\n",
    "        \"\"\" 修改:防止之后要加text \"\"\"\n",
    "        if encoder_hidden_states:\n",
    "            encoder_hidden_states = torch.cat([point_tokens, feature_tokens, encoder_hidden_states], dim=1)\n",
    "        else:\n",
    "            encoder_hidden_states=torch.cat([point_tokens, feature_tokens], dim=1)\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        orig_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "        orig__proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for image_proj_model and adapter_modules\n",
    "        self.vpmatrix_points.load_state_dict(state_dict[\"vpmatrix_points\"], strict=True)\n",
    "        self.atten_modules.load_state_dict(state_dict[\"atten_modules\"], strict=True)\n",
    "        self.image_proj_model.load_state_dict(state_dict[\"image_proj_model\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_VPmatrix_sum = torch.sum(torch.stack([torch.sum(p) for p in self.vpmatrix_points.parameters()]))\n",
    "        new_atten_sum = torch.sum(torch.stack([torch.sum(p) for p in self.atten_modules.parameters()]))\n",
    "        new__proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.image_proj_model.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_VPmatrix_sum != new_VPmatrix_sum, \"Weights of VPmatrixEncoder did not change!\"\n",
    "        assert orig_atten_sum != new_atten_sum, \"Weights of atten_modules did not change!\"\n",
    "        assert orig__proj_sum != new__proj_sum, \"Weights of image_proj_model did not change!\"\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load scheduler, tokenizer and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "    processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "    # freeze parameters of models to save more memory\n",
    "    unet.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    \n",
    "    #vp-matrix encoder\n",
    "    raw_base_points=load_base_points(args.base_point_path)  \n",
    "    vpmatrix_points_sd = VPmatrixPoints(raw_base_points)\n",
    "    image_proj_model = ImageProjModel(\n",
    "        cross_attention_dim=unet.config.cross_attention_dim,\n",
    "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
    "        clip_extra_context_tokens=4,\n",
    "    )\n",
    "    # init pose modules\n",
    "    attn_procs = {}\n",
    "    unet_sd = unet.state_dict()\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        if cross_attention_dim is None:\n",
    "            attn_procs[name] = AttnProcessor()\n",
    "        else:\n",
    "            layer_name = name.split(\".processor\")[0]\n",
    "            weights = {\n",
    "                \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "                \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "            }\n",
    "            attn_procs[name] = PoseAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "            attn_procs[name].load_state_dict(weights)\n",
    "\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    atten_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    \n",
    "    pose_ctrl = posectrl(unet, vpmatrix_points_sd, image_proj_model, atten_modules, args.pretrained_pose_path)\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    #unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    image_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    # optimizer\n",
    "    params_to_opt = itertools.chain(pose_ctrl.vpmatrix_points.parameters(),  pose_ctrl.atten_modules.parameters(), pose_ctrl.image_proj_model.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_opt, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataset = CustomDataset(args.data_root_path)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=args.train_batch_size,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    pose_ctrl, optimizer, train_dataloader = accelerator.prepare(pose_ctrl, optimizer, train_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(0, args.num_train_epochs): #default is 100\n",
    "        begin = time.perf_counter()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            load_data_time = time.perf_counter() - begin\n",
    "            with accelerator.accumulate(pose_ctrl):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"image\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inputs = processor(images=batch['feature'], return_tensors=\"pt\") \n",
    "                    image_tensor = inputs[\"pixel_values\"] \n",
    "                    image_embeds = image_encoder(image_tensor.to(accelerator.device, dtype=weight_dtype)).image_embeds\n",
    "\n",
    "                if \"text_input_ids\" in batch:\n",
    "                    with torch.no_grad():\n",
    "                        encoder_hidden_states = text_encoder(batch[\"text_input_ids\"].to(accelerator.device))[0]\n",
    "                else:\n",
    "                    encoder_hidden_states=None\n",
    "                \n",
    "                noise_pred = pose_ctrl(noisy_latents, timesteps, encoder_hidden_states, batch['view_matrix'], batch['projection_matrix'], image_embeds)\n",
    "        \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            \n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean().item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    print(\"Epoch {}, step {}, data_time: {}, time: {}, step_loss: {}\".format(\n",
    "                        epoch, step, load_data_time, time.perf_counter() - begin, avg_loss))\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % args.save_steps == 0:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                accelerator.save_state(save_path)\n",
    "            \n",
    "            begin = time.perf_counter()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
